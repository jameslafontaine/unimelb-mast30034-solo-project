{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data to cleaned data (renaming columns, data type conversions, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/07/30 15:30:36 WARN Utils: Your hostname, DESKTOP-SATV84A resolves to a loopback address: 127.0.1.1; using 172.26.254.29 instead (on interface eth0)\n",
      "23/07/30 15:30:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/30 15:30:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow Taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for cleaned data\n",
    "\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if not os.path.exists('../data/cleaned/'):\n",
    "    os.makedirs('../data/cleaned/')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc'):\n",
    "    os.makedirs('../data/cleaned/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/yellow'):\n",
    "    os.makedirs('../data/cleaned/tlc/yellow')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/yellow/2022'):\n",
    "    os.makedirs('../data/cleaned/tlc/yellow/2022')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/yellow/2023'):\n",
    "    os.makedirs('../data/cleaned/tlc/yellow/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_raw_dir = '../data/raw/tlc/yellow/'\n",
    "\n",
    "sdf_yellow_2022_01 = spark.read.parquet(f\"{yellow_raw_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_yellow_2022_all = spark.read.parquet(f\"{yellow_raw_dir}2022\")\n",
    "\n",
    "sdf_yellow_2023_01 = spark.read.parquet(f\"{yellow_raw_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_yellow_2023_02 = spark.read.parquet(f\"{yellow_raw_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_yellow_2022_all = spark.read.parquet('../data/raw/yellow/2022')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', DoubleType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_yellow_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', DoubleType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_yellow_2022_all.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_yellow_2023_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_yellow_2023_02.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas appear to be incorrect for all of 2022, so we will adjust all datasets to line up with the schema of Feb 2023 data and ensure consistent lowercase casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdf_yellow_2023_02' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# renaming\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39msdf.withColumnRenamed(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# RatecodeID and passenger_count also need to be converted to int\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m sdf_yellow_2023_02 \u001b[39m=\u001b[39m sdf_yellow_2023_02\u001b[39m.\u001b[39mwithColumn(\n\u001b[1;32m     20\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mratecodeid\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     21\u001b[0m     F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mRatecodeID\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m'\u001b[39m\u001b[39mInteger\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m sdf_yellow_2023_02 \u001b[39m=\u001b[39m sdf_yellow_2023_02\u001b[39m.\u001b[39mwithColumn(\n\u001b[1;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpassenger_count\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m     F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mpassenger_count\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m'\u001b[39m\u001b[39mInteger\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     30\u001b[0m sdf_yellow_2023_02\u001b[39m.\u001b[39mschema\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdf_yellow_2023_02' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''\n",
    "\n",
    "# RatecodeID and passenger_count also need to be converted to int\n",
    "\n",
    "sdf_yellow_2023_02 = sdf_yellow_2023_02.withColumn(\n",
    "    'ratecodeid', \n",
    "    F.col('RatecodeID').cast('Integer') \n",
    ")\n",
    "\n",
    "sdf_yellow_2023_02 = sdf_yellow_2023_02.withColumn(\n",
    "    'passenger_count',\n",
    "    F.col('passenger_count').cast('Integer')\n",
    ")\n",
    "\n",
    "\n",
    "sdf_yellow_2023_02.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('vendorid', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('ratecodeid', IntegerType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we want to ensure everything has consistent casing to make our lives easier\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_yellow_2023_02.columns]\n",
    "sdf_yellow_2023_02 = sdf_yellow_2023_02.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_yellow_2023_02.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert all raw datasets to required schema\n",
    "\n",
    "yellow_raw_dir = '../data/raw/tlc/yellow/'\n",
    "    \n",
    "yellow_clean_dir = '../data/cleaned/tlc/yellow/'\n",
    "\n",
    "# 2022\n",
    "for month in range(1,13):\n",
    "\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{yellow_raw_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema and change casing\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "        .select(*consistent_col_casing)\n",
    "        \n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{yellow_clean_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "# 2023\n",
    "for month in range(1,5):\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{yellow_raw_dir}2023/2023-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema and change casing\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "        .select(*consistent_col_casing)\n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{yellow_clean_dir}2023/2023-{month}.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 1                   \n",
      " tpep_pickup_datetime  | 2022-01-01 00:35:40 \n",
      " tpep_dropoff_datetime | 2022-01-01 00:53:29 \n",
      " passenger_count       | 2                   \n",
      " trip_distance         | 3.8                 \n",
      " ratecodeid            | 1                   \n",
      " store_and_fwd_flag    | N                   \n",
      " pulocationid          | 142                 \n",
      " dolocationid          | 236                 \n",
      " payment_type          | 1                   \n",
      " fare_amount           | 14.5                \n",
      " extra                 | 3.0                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 3.65                \n",
      " tolls_amount          | 0.0                 \n",
      " improvement_surcharge | 0.3                 \n",
      " total_amount          | 21.95               \n",
      " congestion_surcharge  | 2.5                 \n",
      " airport_fee           | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\\nsdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\\n\\nsdf2.show(1, vertical=True, truncate=100)\\n\\nprint(sdf1.count())\\n\\nprint(sdf2.count())\\n\\nsdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\\n\\nsdf.show(1, vertical=True, truncate=100)\\n\\nprint(sdf.count())\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check schema of 2022 cleaned data\n",
    "\n",
    "sdf1 = spark.read.parquet('../data/cleaned/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "#sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check schema of 2023 cleaned data\n",
    "\n",
    "sdf = spark.read.parquet('../data/cleaned/tlc/yellow/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Green Taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for cleaned data\n",
    "\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if not os.path.exists('../data/cleaned/'):\n",
    "    os.makedirs('../data/cleaned/')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc'):\n",
    "    os.makedirs('../data/cleaned/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/green'):\n",
    "    os.makedirs('../data/cleaned/tlc/green')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/green/2022'):\n",
    "    os.makedirs('../data/cleaned/tlc/green/2022')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/green/2023'):\n",
    "    os.makedirs('../data/cleaned/tlc/green/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_raw_dir = '../data/raw/tlc/green/'\n",
    "\n",
    "sdf_green_2022_01 = spark.read.parquet(f\"{green_raw_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_green_2022_all = spark.read.parquet(f\"{green_raw_dir}2022\")\n",
    "\n",
    "sdf_green_2023_01 = spark.read.parquet(f\"{green_raw_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_green_2023_02 = spark.read.parquet(f\"{green_raw_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_green_2022_all = spark.read.parquet('../data/raw/green/2022')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2022_all.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2023_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', LongType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2023_02.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas appear to be incorrect for all of 2022, so we will adjust all datasets to line up with the schema of Feb 2023 data and ensure consistent lowercase casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('ratecodeid', IntegerType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''\n",
    "\n",
    "# RatecodeID and passenger_count also need to be converted to int\n",
    "\n",
    "sdf_green_2023_02 = sdf_green_2023_02.withColumn(\n",
    "    'ratecodeid', \n",
    "    F.col('RatecodeID').cast('Integer') \n",
    ")\n",
    "\n",
    "sdf_green_2023_02 = sdf_green_2023_02.withColumn(\n",
    "    'passenger_count',\n",
    "    F.col('passenger_count').cast('Integer')\n",
    ")\n",
    "\n",
    "\n",
    "sdf_green_2023_02.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('vendorid', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('ratecodeid', IntegerType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we want to ensure everything has consistent casing to make our lives easier\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_green_2023_02.columns]\n",
    "sdf_green_2023_02 = sdf_green_2023_02.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_green_2023_02.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all raw datasets to required schema\n",
    "\n",
    "green_raw_dir = '../data/raw/tlc/green/'\n",
    "    \n",
    "green_clean_dir = '../data/cleaned/tlc/green/'\n",
    "\n",
    "# 2022\n",
    "for month in range(1,13):\n",
    "\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{green_raw_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema and change casing\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "        .select(*consistent_col_casing)\n",
    "        \n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{green_clean_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "# 2023\n",
    "for month in range(1,5):\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{green_raw_dir}2023/2023-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema and change casing\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "        .select(*consistent_col_casing)\n",
    "        \n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{green_clean_dir}2023/2023-{month}.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 1                   \n",
      " lpep_pickup_datetime  | 2022-03-01 00:24:14 \n",
      " lpep_dropoff_datetime | 2022-03-01 00:34:04 \n",
      " store_and_fwd_flag    | N                   \n",
      " ratecodeid            | 1                   \n",
      " pulocationid          | 74                  \n",
      " dolocationid          | 151                 \n",
      " passenger_count       | 1                   \n",
      " trip_distance         | 2.3                 \n",
      " fare_amount           | 9.5                 \n",
      " extra                 | 0.5                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 0.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " ehail_fee             | null                \n",
      " improvement_surcharge | 0.3                 \n",
      " total_amount          | 10.8                \n",
      " payment_type          | 2                   \n",
      " trip_type             | 1                   \n",
      " congestion_surcharge  | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\\nsdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\\n\\nsdf2.show(1, vertical=True, truncate=100)\\n\\nprint(sdf1.count())\\n\\nprint(sdf2.count())\\n\\nsdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\\n\\nsdf.show(1, vertical=True, truncate=100)\\n\\nprint(sdf.count())\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check schema of 2022 cleaned data\n",
    "\n",
    "sdf1 = spark.read.parquet('../data/cleaned/tlc/green/2022/*')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 2                   \n",
      " lpep_pickup_datetime  | 2023-03-01 00:25:10 \n",
      " lpep_dropoff_datetime | 2023-03-01 00:35:47 \n",
      " store_and_fwd_flag    | N                   \n",
      " ratecodeid            | 1                   \n",
      " pulocationid          | 82                  \n",
      " dolocationid          | 196                 \n",
      " passenger_count       | 1                   \n",
      " trip_distance         | 2.36                \n",
      " fare_amount           | 13.5                \n",
      " extra                 | 1.0                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 0.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " ehail_fee             | null                \n",
      " improvement_surcharge | 1.0                 \n",
      " total_amount          | 16.0                \n",
      " payment_type          | 2                   \n",
      " trip_type             | 1                   \n",
      " congestion_surcharge  | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double check schema of 2023 cleaned data\n",
    "\n",
    "sdf = spark.read.parquet('../data/cleaned/tlc/green/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Volume For-Hire Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for cleaned data\n",
    "\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if not os.path.exists('../data/cleaned/'):\n",
    "    os.makedirs('../data/cleaned/')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc'):\n",
    "    os.makedirs('../data/cleaned/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/fhvhv'):\n",
    "    os.makedirs('../data/cleaned/tlc/fhvhv')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/fhvhv/2022'):\n",
    "    os.makedirs('../data/cleaned/tlc/fhvhv/2022')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/fhvhv/2023'):\n",
    "    os.makedirs('../data/cleaned/tlc/fhvhv/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_raw_dir = '../data/raw/tlc/fhvhv/'\n",
    "\n",
    "sdf_fhvhv_2022_01 = spark.read.parquet(f\"{fhvhv_raw_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_fhvhv_2022_all = spark.read.parquet(f\"{fhvhv_raw_dir}2022\")\n",
    "\n",
    "sdf_fhvhv_2023_01 = spark.read.parquet(f\"{fhvhv_raw_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_fhvhv_2023_02 = spark.read.parquet(f\"{fhvhv_raw_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_fhvhv_2022_all = spark.read.parquet(f\"{fhvhv_raw_dir}2022\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2022_all.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2023_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2023_02.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas appear to be incorrect for all of 2022, so we will adjust all datasets to line up with the schema of Feb 2023 data and ensure consistent lowercase casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', IntegerType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''\n",
    "\n",
    "# trip_time needs to be converted to integer\n",
    "\n",
    "sdf_fhvhv_2023_02 = sdf_fhvhv_2023_02.withColumn(\n",
    "    'trip_time', \n",
    "    F.col('trip_time').cast('Integer') \n",
    ")\n",
    "\n",
    "sdf_fhvhv_2023_02.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', IntegerType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we want to ensure everything has consistent casing to make our lives easier\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_fhvhv_2023_02.columns]\n",
    "sdf_fhvhv_2023_02 = sdf_fhvhv_2023_02.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_fhvhv_2023_02.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert all raw datasets to required schema\n",
    "\n",
    "fhvhv_raw_dir = '../data/raw/tlc/fhvhv/'\n",
    "    \n",
    "fhvhv_clean_dir = '../data/cleaned/tlc/fhvhv/'\n",
    "\n",
    "# 2022\n",
    "for month in range(1,13):\n",
    "\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{fhvhv_raw_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema and change casing\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "        .select(*consistent_col_casing)\n",
    "        \n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{fhvhv_clean_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "# 2023\n",
    "for month in range(1,5):\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{fhvhv_raw_dir}2023/2023-{month}.parquet\")\n",
    " \n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema and change casing\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "        .select(*consistent_col_casing)\n",
    "        \n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{fhvhv_clean_dir}2023/2023-{month}.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " hvfhs_license_num    | HV0003              \n",
      " dispatching_base_num | B03404              \n",
      " originating_base_num | B03404              \n",
      " request_datetime     | 2022-01-01 00:05:31 \n",
      " on_scene_datetime    | 2022-01-01 00:05:40 \n",
      " pickup_datetime      | 2022-01-01 00:07:24 \n",
      " dropoff_datetime     | 2022-01-01 00:18:28 \n",
      " pulocationid         | 170                 \n",
      " dolocationid         | 161                 \n",
      " trip_miles           | 1.18                \n",
      " trip_time            | 664                 \n",
      " base_passenger_fare  | 24.9                \n",
      " tolls                | 0.0                 \n",
      " bcf                  | 0.75                \n",
      " sales_tax            | 2.21                \n",
      " congestion_surcharge | 2.75                \n",
      " airport_fee          | 0.0                 \n",
      " tips                 | 0.0                 \n",
      " driver_pay           | 23.03               \n",
      " shared_request_flag  | N                   \n",
      " shared_match_flag    | N                   \n",
      " access_a_ride_flag   |                     \n",
      " wav_request_flag     | N                   \n",
      " wav_match_flag       | N                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\\nsdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\\n\\nsdf2.show(1, vertical=True, truncate=100)\\n\\nprint(sdf1.count())\\n\\nprint(sdf2.count())\\n\\nsdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\\n\\nsdf.show(1, vertical=True, truncate=100)\\n\\nprint(sdf.count())\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check schema of 2022 cleaned data\n",
    "\n",
    "sdf1 = spark.read.parquet('../data/cleaned/tlc/fhvhv/2022/*')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " hvfhs_license_num    | HV0003              \n",
      " dispatching_base_num | B03404              \n",
      " originating_base_num | B03404              \n",
      " request_datetime     | 2023-01-01 00:18:06 \n",
      " on_scene_datetime    | 2023-01-01 00:19:24 \n",
      " pickup_datetime      | 2023-01-01 00:19:38 \n",
      " dropoff_datetime     | 2023-01-01 00:48:07 \n",
      " pulocationid         | 48                  \n",
      " dolocationid         | 68                  \n",
      " trip_miles           | 0.94                \n",
      " trip_time            | 1709                \n",
      " base_passenger_fare  | 25.95               \n",
      " tolls                | 0.0                 \n",
      " bcf                  | 0.78                \n",
      " sales_tax            | 2.3                 \n",
      " congestion_surcharge | 2.75                \n",
      " airport_fee          | 0.0                 \n",
      " tips                 | 5.22                \n",
      " driver_pay           | 27.83               \n",
      " shared_request_flag  | N                   \n",
      " shared_match_flag    | N                   \n",
      " access_a_ride_flag   |                     \n",
      " wav_request_flag     | N                   \n",
      " wav_match_flag       | N                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double check schema of 2023 cleaned data\n",
    "\n",
    "sdf = spark.read.parquet('../data/cleaned/tlc/fhvhv/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for cleaned data\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/cleaned/'):\n",
    "    os.makedirs('../data/cleaned/')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc'):\n",
    "    os.makedirs('../data/cleaned/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/taxi_zones'):\n",
    "    os.makedirs('../data/cleaned/tlc/taxi_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_lookup = spark.read.option(\"header\",True) \\\n",
    "                   .csv(\"../data/raw/tlc/taxi_zones/taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n",
      "StructType([StructField('LocationID', StringType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(taxi_zone_lookup.limit(5))\n",
    "\n",
    "print(taxi_zone_lookup.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just need to convert the column names to lowercase for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|locationid|      borough|                zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n",
      "StructType([StructField('locationid', StringType(), True), StructField('borough', StringType(), True), StructField('zone', StringType(), True), StructField('service_zone', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# converting to lower case\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in taxi_zone_lookup.columns]\n",
    "taxi_zone_lookup = taxi_zone_lookup.select(*consistent_col_casing)\n",
    "\n",
    "print(taxi_zone_lookup.limit(5))\n",
    "print(taxi_zone_lookup.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save cleaned data as parquet\n",
    "taxi_zone_lookup \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/cleaned/tlc/taxi_zones/taxi_zone_lookup.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of +----------+-------------+--------------------+------------+\n",
       "|locationid|      borough|                zone|service_zone|\n",
       "+----------+-------------+--------------------+------------+\n",
       "|         1|          EWR|      Newark Airport|         EWR|\n",
       "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
       "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
       "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
       "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
       "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
       "|         7|       Queens|             Astoria|   Boro Zone|\n",
       "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
       "|         9|       Queens|          Auburndale|   Boro Zone|\n",
       "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
       "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
       "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
       "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
       "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
       "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
       "|        16|       Queens|             Bayside|   Boro Zone|\n",
       "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
       "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
       "|        19|       Queens|           Bellerose|   Boro Zone|\n",
       "|        20|        Bronx|             Belmont|   Boro Zone|\n",
       "+----------+-------------+--------------------+------------+\n",
       "only showing top 20 rows\n",
       ">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned data\n",
    "taxi_zone_lookup = spark.read.parquet('../data/cleaned/tlc/taxi_zones/taxi_zone_lookup.parquet')\n",
    "\n",
    "taxi_zone_lookup.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move taxi_zones.zip to cleaned data\n",
    "import shutil\n",
    "\n",
    "src = '../data/raw/tlc/taxi_zones/taxi_zones.zip'\n",
    "\n",
    "dst = '../data/cleaned/tlc/taxi_zones/taxi_zones.zip'\n",
    "\n",
    "shutil.copyfile(src, dst)\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# unzip shapefile\n",
    "with zipfile.ZipFile(dst, 'r') as zip_ref:\n",
    "    zip_ref.extractall('../data/cleaned/tlc/taxi_zones')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
