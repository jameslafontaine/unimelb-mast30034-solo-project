{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landing Layer to Raw Layer (renaming columns, data type conversions, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/01 19:51:27 WARN Utils: Your hostname, DESKTOP-SATV84A resolves to a loopback address: 127.0.1.1; using 172.26.254.29 instead (on interface eth0)\n",
      "23/08/01 19:51:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/01 19:51:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/01 19:51:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow Taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for raw layer\n",
    "\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if not os.path.exists('../data/raw/'):\n",
    "    os.makedirs('../data/raw/')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc'):\n",
    "    os.makedirs('../data/raw/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/yellow'):\n",
    "    os.makedirs('../data/raw/tlc/yellow')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/yellow/2021'):\n",
    "    os.makedirs('../data/raw/tlc/yellow/2021')   \n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/yellow/2022'):\n",
    "    os.makedirs('../data/raw/tlc/yellow/2022')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/yellow/2023'):\n",
    "    os.makedirs('../data/raw/tlc/yellow/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_landing_dir = '../data/landing/tlc/yellow/'\n",
    "\n",
    "sdf_yellow_2022_01 = spark.read.parquet(f\"{yellow_landing_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_yellow_2022_all = spark.read.parquet(f\"{yellow_landing_dir}2022\")\n",
    "\n",
    "sdf_yellow_2023_01 = spark.read.parquet(f\"{yellow_landing_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_yellow_2023_02 = spark.read.parquet(f\"{yellow_landing_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_yellow_2022_all = spark.read.parquet('../data/raw/yellow/2022')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', DoubleType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_yellow_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', DoubleType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_yellow_2022_all.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_yellow_2023_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_yellow_2023_02.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas appear to be incorrect for all of 2022, so we will adjust all datasets to line up with the schema of Feb 2023 data and ensure consistent lowercase casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('ratecodeid', IntegerType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('Airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''\n",
    "\n",
    "# RatecodeID and passenger_count also need to be converted to int\n",
    "\n",
    "sdf_yellow_2023_02 = sdf_yellow_2023_02.withColumn(\n",
    "    'ratecodeid', \n",
    "    F.col('RatecodeID').cast('Integer') \n",
    ")\n",
    "\n",
    "sdf_yellow_2023_02 = sdf_yellow_2023_02.withColumn(\n",
    "    'passenger_count',\n",
    "    F.col('passenger_count').cast('Integer')\n",
    ")\n",
    "\n",
    "\n",
    "sdf_yellow_2023_02.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('vendorid', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('ratecodeid', IntegerType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we want to ensure everything has consistent casing to make our lives easier\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_yellow_2023_02.columns]\n",
    "sdf_yellow_2023_02 = sdf_yellow_2023_02.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_yellow_2023_02.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert all landing datasets to required schema\n",
    "\n",
    "yellow_landing_dir = '../data/landing/tlc/yellow/'\n",
    "    \n",
    "yellow_raw_dir = '../data/raw/tlc/yellow/'\n",
    "\n",
    "YEARS = ['2021', '2022', '2023']\n",
    "\n",
    "for year in YEARS:\n",
    "    if (year == '2021'):\n",
    "        MONTHS = [12]\n",
    "    if (year == '2022'):\n",
    "        MONTHS = range(1,13)\n",
    "    if (year == '2023'):   \n",
    "        MONTHS = range(1,5) \n",
    "    for month in MONTHS:\n",
    "\n",
    "        month = str(month).zfill(2) \n",
    "\n",
    "        sdf_malformed = spark.read.parquet(f\"{yellow_landing_dir}{year}/{year}-{month}.parquet\")\n",
    "\n",
    "        # select all columns from the existing malformed dataframe and cast it to the required schema and change casing, drop ehail_fee as it has only NULL values\n",
    "        sdf_malformed = sdf_malformed \\\n",
    "            .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "            .select(*consistent_col_casing) \\\n",
    "            .drop('ehail_fee')\n",
    "        \n",
    "        sdf_malformed \\\n",
    "        .coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet(f\"{yellow_raw_dir}{year}/{year}-{month}.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 1                   \n",
      " tpep_pickup_datetime  | 2022-01-01 00:35:40 \n",
      " tpep_dropoff_datetime | 2022-01-01 00:53:29 \n",
      " passenger_count       | 2                   \n",
      " trip_distance         | 3.8                 \n",
      " ratecodeid            | 1                   \n",
      " store_and_fwd_flag    | N                   \n",
      " pulocationid          | 142                 \n",
      " dolocationid          | 236                 \n",
      " payment_type          | 1                   \n",
      " fare_amount           | 14.5                \n",
      " extra                 | 3.0                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 3.65                \n",
      " tolls_amount          | 0.0                 \n",
      " improvement_surcharge | 0.3                 \n",
      " total_amount          | 21.95               \n",
      " congestion_surcharge  | 2.5                 \n",
      " airport_fee           | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\\nsdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\\n\\nsdf2.show(1, vertical=True, truncate=100)\\n\\nprint(sdf1.count())\\n\\nprint(sdf2.count())\\n\\nsdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\\n\\nsdf.show(1, vertical=True, truncate=100)\\n\\nprint(sdf.count())\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check schema of 2022 raw layer data\n",
    "\n",
    "sdf1 = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "#sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checkinglanding vs raw dataset shape and landing dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check schema of 2023 raw layer data\n",
    "\n",
    "sdf = spark.read.parquet('../data/cleaned/tlc/yellow/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Green Taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for raw layer data\n",
    "\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if not os.path.exists('../data/raw/'):\n",
    "    os.makedirs('../data/raw/')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc'):\n",
    "    os.makedirs('../data/raw/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/green'):\n",
    "    os.makedirs('../data/raw/tlc/green')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/green/2022'):\n",
    "    os.makedirs('../data/raw/tlc/green/2022')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/green/2023'):\n",
    "    os.makedirs('../data/raw/tlc/green/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_landing_dir = '../data/landing/tlc/green/'\n",
    "\n",
    "sdf_green_2022_01 = spark.read.parquet(f\"{green_landing_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_green_2022_all = spark.read.parquet(f\"{green_landing_dir}2022\")\n",
    "\n",
    "sdf_green_2023_01 = spark.read.parquet(f\"{green_landing_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_green_2023_02 = spark.read.parquet(f\"{green_landing_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_green_2022_all = spark.read.parquet('../data/raw/green/2022')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2022_all.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2023_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', LongType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2023_02.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       1| 2023-02-01 00:46:22|  2023-02-01 01:05:57|                 N|         1|          74|         265|              1|         10.8|       42.9|  1.0|    1.5|       0.0|         0.0|     null|                  1.0|        45.4|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:05:09|  2023-02-01 00:22:42|                 N|         1|         216|         196|              1|         4.76|       23.3|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        25.8|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:03:47|  2023-02-01 00:27:30|                 N|         1|           7|         114|              1|         6.32|       30.3|  1.0|    0.5|      8.89|         0.0|     null|                  1.0|       44.44|           1|        1|                2.75|\n",
      "|       2| 2023-01-31 23:30:56|  2023-01-31 23:51:40|                 N|         1|          74|         239|              1|          3.5|       16.3|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|       21.55|           2|        1|                2.75|\n",
      "|       2| 2023-02-01 00:15:05|  2023-02-01 00:26:02|                 N|         1|          82|         223|              1|         3.14|       17.0|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        19.5|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:16:10|  2023-02-01 00:16:13|                 N|         5|           7|           7|              1|          0.0|       10.0|  0.0|    0.0|      2.06|         0.0|     null|                  0.3|       12.36|           1|        2|                 0.0|\n",
      "|       2| 2023-02-01 00:41:02|  2023-02-01 00:52:02|                 N|         1|          92|         121|              1|         2.52|       14.2|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        16.7|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:44:48|  2023-02-01 00:56:06|                 N|         1|         244|          42|              1|         1.96|       13.5|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        16.0|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:43:54|  2023-02-01 00:49:35|                 N|         1|          75|          41|              1|         1.46|        8.6|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        11.1|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:34:40|  2023-02-01 00:45:31|                 N|         1|          95|         121|              1|         2.38|       14.9|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        17.4|           1|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:50:01|  2023-02-01 00:54:29|                 N|         1|          97|          97|              2|         0.88|        7.2|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|         9.7|           2|        1|                 0.0|\n",
      "|       2| 2023-01-31 23:59:57|  2023-02-01 00:04:25|                 N|         1|          75|          74|              1|         0.83|        7.2|  1.0|    0.5|      2.91|         0.0|     null|                  1.0|       12.61|           1|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:46:33|  2023-02-01 01:08:11|                 N|         1|         112|         188|              2|         5.25|       24.7|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        27.2|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:02:46|  2023-02-01 00:09:11|                 N|         1|          95|         135|              1|         1.43|        9.3|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        11.8|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:38:31|  2023-02-01 00:41:51|                 N|         1|          95|          95|              1|         0.51|        5.1|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|         7.6|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:09:41|  2023-02-01 00:10:22|                 N|         1|          95|          95|              1|         0.07|        3.0|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|         5.5|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:42:37|  2023-02-01 00:56:54|                 N|         1|          92|         129|              1|         3.02|       17.0|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        19.5|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:29:29|  2023-02-01 00:44:49|                 N|         1|          82|         146|              1|         2.98|       17.7|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        20.2|           2|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:25:36|  2023-02-01 00:34:48|                 N|         1|          95|         135|              1|          2.4|       12.8|  1.0|    0.5|      3.06|         0.0|     null|                  1.0|       18.36|           1|        1|                 0.0|\n",
      "|       2| 2023-02-01 00:42:41|  2023-02-01 00:49:56|                 N|         1|          55|          55|              1|         0.29|        7.9|  1.0|    0.5|       0.0|         0.0|     null|                  1.0|        10.4|           2|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_green_2023_02.select('ehail_fee').distinct().collect()\n",
    "\n",
    "sdf_green_2023_02.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas appear to be incorrect for all of 2022, so we will adjust all datasets to line up with the schema of Feb 2023 data and ensure consistent lowercase casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('ratecodeid', IntegerType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''\n",
    "\n",
    "# RatecodeID and passenger_count also need to be converted to int\n",
    "\n",
    "sdf_green_2023_02 = sdf_green_2023_02.withColumn(\n",
    "    'ratecodeid', \n",
    "    F.col('RatecodeID').cast('Integer') \n",
    ")\n",
    "\n",
    "sdf_green_2023_02 = sdf_green_2023_02.withColumn(\n",
    "    'passenger_count',\n",
    "    F.col('passenger_count').cast('Integer')\n",
    ")\n",
    "\n",
    "\n",
    "sdf_green_2023_02.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('vendorid', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('ratecodeid', IntegerType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we want to ensure everything has consistent casing to make our lives easier\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_green_2023_02.columns]\n",
    "sdf_green_2023_02 = sdf_green_2023_02.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_green_2023_02.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all landing datasets to required schema\n",
    "\n",
    "green_landing_dir = '../data/landing/tlc/green/'\n",
    "    \n",
    "green_raw_dir = '../data/raw/tlc/green/'\n",
    "\n",
    "YEARS = ['2021', '2022', '2023']\n",
    "\n",
    "for year in YEARS:\n",
    "    if (year == '2021'):\n",
    "        MONTHS = [12]\n",
    "    if (year == '2022'):\n",
    "        MONTHS = range(1,13)\n",
    "    if (year == '2023'):   \n",
    "        MONTHS = range(1,5) \n",
    "    for month in MONTHS:\n",
    "\n",
    "        month = str(month).zfill(2) \n",
    "\n",
    "        sdf_malformed = spark.read.parquet(f\"{green_landing_dir}{year}/{year}-{month}.parquet\")\n",
    "\n",
    "        # select all columns from the existing malformed dataframe and cast it to the required schema and change casing, drop ehail_fee as it has only NULL values\n",
    "        sdf_malformed = sdf_malformed \\\n",
    "            .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)]) \\\n",
    "            .select(*consistent_col_casing) \\\n",
    "            .drop('ehail_fee')\n",
    "        \n",
    "        sdf_malformed \\\n",
    "        .coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet(f\"{green_raw_dir}{year}/{year}-{month}.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 1                   \n",
      " lpep_pickup_datetime  | 2022-03-01 00:24:14 \n",
      " lpep_dropoff_datetime | 2022-03-01 00:34:04 \n",
      " store_and_fwd_flag    | N                   \n",
      " ratecodeid            | 1                   \n",
      " pulocationid          | 74                  \n",
      " dolocationid          | 151                 \n",
      " passenger_count       | 1                   \n",
      " trip_distance         | 2.3                 \n",
      " fare_amount           | 9.5                 \n",
      " extra                 | 0.5                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 0.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " improvement_surcharge | 0.3                 \n",
      " total_amount          | 10.8                \n",
      " payment_type          | 2                   \n",
      " trip_type             | 1                   \n",
      " congestion_surcharge  | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\\nsdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\\n\\nsdf2.show(1, vertical=True, truncate=100)\\n\\nprint(sdf1.count())\\n\\nprint(sdf2.count())\\n\\nsdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\\n\\nsdf.show(1, vertical=True, truncate=100)\\n\\nprint(sdf.count())\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check schema of 2022 raw layer data\n",
    "\n",
    "sdf1 = spark.read.parquet('../data/raw/tlc/green/2022/*')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checking landing vs raw dataset shape and landing dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 2                   \n",
      " lpep_pickup_datetime  | 2023-03-01 00:25:10 \n",
      " lpep_dropoff_datetime | 2023-03-01 00:35:47 \n",
      " store_and_fwd_flag    | N                   \n",
      " ratecodeid            | 1                   \n",
      " pulocationid          | 82                  \n",
      " dolocationid          | 196                 \n",
      " passenger_count       | 1                   \n",
      " trip_distance         | 2.36                \n",
      " fare_amount           | 13.5                \n",
      " extra                 | 1.0                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 0.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " improvement_surcharge | 1.0                 \n",
      " total_amount          | 16.0                \n",
      " payment_type          | 2                   \n",
      " trip_type             | 1                   \n",
      " congestion_surcharge  | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double check schema of 2023 raw layer data\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/green/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for raw layer\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/raw/'):\n",
    "    os.makedirs('../data/raw/')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc'):\n",
    "    os.makedirs('../data/raw/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/raw/tlc/taxi_zones'):\n",
    "    os.makedirs('../data/raw/tlc/taxi_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_lookup = spark.read.option(\"header\",True) \\\n",
    "                   .csv(\"../data/landing/tlc/taxi_zones/taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n",
      "StructType([StructField('LocationID', StringType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(taxi_zone_lookup.limit(5))\n",
    "\n",
    "print(taxi_zone_lookup.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just need to convert the column names to lowercase for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|locationid|      borough|                zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n",
      "StructType([StructField('locationid', StringType(), True), StructField('borough', StringType(), True), StructField('zone', StringType(), True), StructField('service_zone', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# converting to lower case\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in taxi_zone_lookup.columns]\n",
    "taxi_zone_lookup = taxi_zone_lookup.select(*consistent_col_casing)\n",
    "\n",
    "print(taxi_zone_lookup.limit(5))\n",
    "print(taxi_zone_lookup.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save raw layer data as parquet\n",
    "taxi_zone_lookup \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/raw/tlc/taxi_zones/taxi_zone_lookup.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of +----------+-------------+--------------------+------------+\n",
       "|locationid|      borough|                zone|service_zone|\n",
       "+----------+-------------+--------------------+------------+\n",
       "|         1|          EWR|      Newark Airport|         EWR|\n",
       "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
       "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
       "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
       "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
       "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
       "|         7|       Queens|             Astoria|   Boro Zone|\n",
       "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
       "|         9|       Queens|          Auburndale|   Boro Zone|\n",
       "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
       "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
       "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
       "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
       "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
       "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
       "|        16|       Queens|             Bayside|   Boro Zone|\n",
       "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
       "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
       "|        19|       Queens|           Bellerose|   Boro Zone|\n",
       "|        20|        Bronx|             Belmont|   Boro Zone|\n",
       "+----------+-------------+--------------------+------------+\n",
       "only showing top 20 rows\n",
       ">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check raw layer data\n",
    "taxi_zone_lookup = spark.read.parquet('../data/raw/tlc/taxi_zones/taxi_zone_lookup.parquet')\n",
    "\n",
    "taxi_zone_lookup.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move taxi_zones.zip to raw layer data\n",
    "import shutil\n",
    "\n",
    "src = '../data/landing/tlc/taxi_zones/taxi_zones.zip'\n",
    "\n",
    "dst = '../data/raw/tlc/taxi_zones/taxi_zones.zip'\n",
    "\n",
    "shutil.copyfile(src, dst)\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# unzip shapefile\n",
    "with zipfile.ZipFile(dst, 'r') as zip_ref:\n",
    "    zip_ref.extractall('../data/raw/tlc/taxi_zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motor Vehicle Collisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subway Entrances and Exits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subway Hourly Ridership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bus Hourly Ridership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotel Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airbnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
