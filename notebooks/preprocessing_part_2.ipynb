{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark (load in the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/07/27 05:21:49 WARN Utils: Your hostname, JamesPanzera resolves to a loopback address: 127.0.1.1; using 172.28.10.204 instead (on interface eth0)\n",
      "23/07/27 05:21:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/27 05:21:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/27 05:21:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_yellow_2022_01 = spark.read.parquet('../data/raw/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf_yellow_2022_all = spark.read.parquet('../data/raw/yellow/2022')\n",
    "\n",
    "#sdf_green_all = spark.read.parquet('../data/raw/green')\n",
    "\n",
    "#sdf_fhv_all = spark.read.parquet('../data/raw/fhv')\n",
    "\n",
    "#sdf_fhvhv_all = spark.read.parquet('../data/raw/fhvhv')\n",
    "\n",
    "# may not be feasible to combine the datasets for different vehicle types, double check data dictionaries first\n",
    "\n",
    "#sdf_all = spark.read.parquet('../data/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:35:40</td><td>2022-01-01 00:53:29</td><td>2.0</td><td>3.8</td><td>1.0</td><td>N</td><td>142</td><td>236</td><td>1</td><td>14.5</td><td>3.0</td><td>0.5</td><td>3.65</td><td>0.0</td><td>0.3</td><td>21.95</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:33:43</td><td>2022-01-01 00:42:07</td><td>1.0</td><td>2.1</td><td>1.0</td><td>N</td><td>236</td><td>42</td><td>1</td><td>8.0</td><td>0.5</td><td>0.5</td><td>4.0</td><td>0.0</td><td>0.3</td><td>13.3</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:53:21</td><td>2022-01-01 01:02:19</td><td>1.0</td><td>0.97</td><td>1.0</td><td>N</td><td>166</td><td>166</td><td>1</td><td>7.5</td><td>0.5</td><td>0.5</td><td>1.76</td><td>0.0</td><td>0.3</td><td>10.56</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:25:21</td><td>2022-01-01 00:35:23</td><td>1.0</td><td>1.09</td><td>1.0</td><td>N</td><td>114</td><td>68</td><td>2</td><td>8.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>11.8</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:36:48</td><td>2022-01-01 01:14:20</td><td>1.0</td><td>4.3</td><td>1.0</td><td>N</td><td>68</td><td>163</td><td>1</td><td>23.5</td><td>0.5</td><td>0.5</td><td>3.0</td><td>0.0</td><td>0.3</td><td>30.3</td><td>2.5</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       1| 2022-01-01 00:35:40|  2022-01-01 00:53:29|            2.0|          3.8|       1.0|                 N|         142|         236|           1|       14.5|  3.0|    0.5|      3.65|         0.0|                  0.3|       21.95|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:33:43|  2022-01-01 00:42:07|            1.0|          2.1|       1.0|                 N|         236|          42|           1|        8.0|  0.5|    0.5|       4.0|         0.0|                  0.3|        13.3|                 0.0|        0.0|\n",
       "|       2| 2022-01-01 00:53:21|  2022-01-01 01:02:19|            1.0|         0.97|       1.0|                 N|         166|         166|           1|        7.5|  0.5|    0.5|      1.76|         0.0|                  0.3|       10.56|                 0.0|        0.0|\n",
       "|       2| 2022-01-01 00:25:21|  2022-01-01 00:35:23|            1.0|         1.09|       1.0|                 N|         114|          68|           2|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|        0.0|\n",
       "|       2| 2022-01-01 00:36:48|  2022-01-01 01:14:20|            1.0|          4.3|       1.0|                 N|          68|         163|           1|       23.5|  0.5|    0.5|       3.0|         0.0|                  0.3|        30.3|                 2.5|        0.0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_yellow_2022_01.limit(5)\n",
    "\n",
    "#sdf_green_all.limit(5)\n",
    "\n",
    "#sdf_fhv_all.limit(5)\n",
    "\n",
    "#sdf_fhvhv_all.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dataset count (shape)\n",
    "#sdf_yellow_2022_01.count(), sdf_yellow_2022_all.count()\n",
    "len(sdf_yellow_2022_01.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 10 records / rows for passenger count and trip distance attributes\n",
    "sdf_yellow_2022_01.select('passenger_count', 'trip_distance').limit(10) \n",
    "\n",
    "# access passenger count\n",
    "sdf_yellow_2022_01['passenger_count']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# F.col('passenger_count') # column type for use with sdf.filter(F.col('passenger_count') == 5).limit(5)) for example\n",
    "\n",
    "# this example below retrieves records where passenger count and trip distance are non-zero\n",
    "''' sdf.where(\n",
    "    # != 0 is fine, but let's also take into account some more realistic filters\n",
    "    (F.col('passenger_count') > 0) \n",
    "    & (F.col('trip_distance') > 0)\n",
    ") '''\n",
    "\n",
    "# sdf.groupby('passenger_count').mean('trip_distance').limit(5) # groups by passenger count and average trip distance for each\n",
    "\n",
    "# group by passenger count and average trip amount for each passenger count and show max trip distance for each passenger count\n",
    "# use alias to give a different name to the attribute columns in the aggregated results\n",
    "''' aggregated_results = sdf \\\n",
    "                    .groupBy(\"passenger_count\") \\\n",
    "                    .agg(\n",
    "                        F.mean(\"total_amount\").alias(\"avg_trip_amount_usd\"),\n",
    "                        F.max(\"trip_distance\").alias(\"max_trip_distance_miles\")\n",
    "                    ) \\\n",
    "                    .orderBy(\"passenger_count\")\n",
    "\n",
    "aggregated_results.show()\n",
    "\n",
    "'''\n",
    "\n",
    "# save dataframe as parquet\n",
    "# aggregated_results.write.mode('overwrite').parquet('../../data/tute_data/aggregated_results') \n",
    "\n",
    "# read parquet back into dataframe\n",
    "# temp_results = spark.read.parquet('../../data/tute_data/aggregated_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample your data and convert it into a `pandas` dataframe, you can use the `.toPandas()` and save a sample of the `sdf` to read it in. We will also fix the random seed to be `0` just for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 0.05\n",
    "\n",
    "df = sdf.sample(SAMPLE_SIZE, seed=0).toPandas()\n",
    "\n",
    "df.to_parquet('../../data/tute_data/sample_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter / transform enriched data using business logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove certain features / records based on business logic / data limitations and remove certain values (e.g. NULL) \n",
    "\n",
    "# Discretise numeric features or convert categorical / ordinal features to numeric features depending on what model will be utilised, may also\n",
    "# need to standardise / scale numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you save every dataframe or aggregation as `parquet` so you don't keep running your notebook from top to bottom waiting 20 years for a result, or have so many variables and dataframes defined that you run out of memory for small transformations.\n",
    "\n",
    "We strongly suggest you have a `code` folder in your Project 1 directory with the following structure:\n",
    "- `preprocessing_notebook_part_1.ipynb`: outputs a structured parquet format and saves it.\n",
    "- `preprocessing_notebook_part_2.ipynb`: reads in the output from above and does some aggregations and sampling before saving it.\n",
    "- `data_analysis_xyz.ipynb`: conducts analysis on a single sample or aggregation from the output above.\n",
    "- `data_analysis_abc.ipynb`: conducts analysis on another single sample or aggregation from the output above.\n",
    "- `...`\n",
    "\n",
    "This is a very basic version of what you call a \"data pipeline\" (or ETL pipeline, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read business rules and particular limitations of the data carefully (e.g. tip amount not recorded for cash payment) to guide self in determining which attributes may need to be filtered / removed or transformed / engineered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
