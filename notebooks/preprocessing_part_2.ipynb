{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned data to curated data (filter and transform using business logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/07/30 15:44:03 WARN Utils: Your hostname, DESKTOP-SATV84A resolves to a loopback address: 127.0.1.1; using 172.26.254.29 instead (on interface eth0)\n",
      "23/07/30 15:44:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/30 15:44:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/30 15:44:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow Taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for curated data\n",
    "\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if not os.path.exists('../data/curated/'):\n",
    "    os.makedirs('../data/curated/')\n",
    "    \n",
    "if not os.path.exists('../data/curated/tlc'):\n",
    "    os.makedirs('../data/curated/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/curated/tlc/yellow'):\n",
    "    os.makedirs('../data/curated/tlc/yellow')\n",
    "    \n",
    "if not os.path.exists('../data/curated/tlc/yellow/2022'):\n",
    "    os.makedirs('../data/curated/tlc/yellow/2022')\n",
    "    \n",
    "if not os.path.exists('../data/curated/tlc/yellow/2023'):\n",
    "    os.makedirs('../data/curated/tlc/yellow/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_cleaned_dir = '../data/cleaned/tlc/yellow/'\n",
    "\n",
    "yellow_curated_dir = '../data/curated/tlc/yellow/'\n",
    "\n",
    "sdf_yellow_2022_01 = spark.read.parquet(f\"{yellow_cleaned_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_yellow_2022_all = spark.read.parquet(f\"{yellow_cleaned_dir}2022/*\")\n",
    "\n",
    "sdf_yellow_2023_01 = spark.read.parquet(f\"{yellow_cleaned_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_yellow_2023_02 = spark.read.parquet(f\"{yellow_cleaned_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_yellow_2022_all = spark.read.parquet('../data/cleaned/yellow/2022')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('vendorid', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('ratecodeid', IntegerType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_yellow_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>vendorid</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>ratecodeid</th><th>store_and_fwd_flag</th><th>pulocationid</th><th>dolocationid</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>2</td><td>2022-01-01 02:39:26</td><td>2022-01-01 02:45:53</td><td>1</td><td>1.73</td><td>1</td><td>N</td><td>137</td><td>229</td><td>2</td><td>-7.5</td><td>-0.5</td><td>-0.5</td><td>3.0</td><td>0.0</td><td>-0.3</td><td>-8.3</td><td>-2.5</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 05:37:52</td><td>2022-01-01 06:10:00</td><td>2</td><td>8.37</td><td>1</td><td>N</td><td>90</td><td>7</td><td>2</td><td>-27.5</td><td>-0.5</td><td>-0.5</td><td>9.39</td><td>0.0</td><td>-0.3</td><td>-21.91</td><td>-2.5</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 16:40:12</td><td>2022-01-01 17:16:57</td><td>2</td><td>18.17</td><td>2</td><td>N</td><td>132</td><td>48</td><td>2</td><td>-52.0</td><td>0.0</td><td>-0.5</td><td>12.37</td><td>-6.55</td><td>-0.3</td><td>-50.73</td><td>-2.5</td><td>-1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 17:53:58</td><td>2022-01-01 18:14:15</td><td>1</td><td>2.3</td><td>5</td><td>N</td><td>164</td><td>142</td><td>2</td><td>13.3</td><td>0.0</td><td>0.5</td><td>2.66</td><td>0.0</td><td>0.3</td><td>19.26</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 17:19:45</td><td>2022-01-01 17:33:03</td><td>1</td><td>2.25</td><td>1</td><td>N</td><td>170</td><td>236</td><td>2</td><td>-11.5</td><td>0.0</td><td>-0.5</td><td>2.96</td><td>0.0</td><td>-0.3</td><td>-11.84</td><td>-2.5</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|vendorid|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|ratecodeid|store_and_fwd_flag|pulocationid|dolocationid|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       2| 2022-01-01 02:39:26|  2022-01-01 02:45:53|              1|         1.73|         1|                 N|         137|         229|           2|       -7.5| -0.5|   -0.5|       3.0|         0.0|                 -0.3|        -8.3|                -2.5|        0.0|\n",
       "|       2| 2022-01-01 05:37:52|  2022-01-01 06:10:00|              2|         8.37|         1|                 N|          90|           7|           2|      -27.5| -0.5|   -0.5|      9.39|         0.0|                 -0.3|      -21.91|                -2.5|        0.0|\n",
       "|       2| 2022-01-01 16:40:12|  2022-01-01 17:16:57|              2|        18.17|         2|                 N|         132|          48|           2|      -52.0|  0.0|   -0.5|     12.37|       -6.55|                 -0.3|      -50.73|                -2.5|      -1.25|\n",
       "|       2| 2022-01-01 17:53:58|  2022-01-01 18:14:15|              1|          2.3|         5|                 N|         164|         142|           2|       13.3|  0.0|    0.5|      2.66|         0.0|                  0.3|       19.26|                 2.5|        0.0|\n",
       "|       2| 2022-01-01 17:19:45|  2022-01-01 17:33:03|              1|         2.25|         1|                 N|         170|         236|           2|      -11.5|  0.0|   -0.5|      2.96|         0.0|                 -0.3|      -11.84|                -2.5|        0.0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sdf_yellow_2022_01.show(5)\n",
    "\n",
    "sdf_yellow_2022_01.where(\n",
    "    (F.col('payment_type') == 2)\n",
    "    & (F.col('tip_amount') > 0)\n",
    "    & (F.col('tip_amount') > 0)\n",
    "    ).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips: \n",
      "2463931\n",
      "Credit card payment trips with valid total: \n",
      "1039104\n",
      "Trips with valid total: \n",
      "1399649\n",
      "Cash payment trips with valid total: \n",
      "348632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>vendorid</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>ratecodeid</th><th>store_and_fwd_flag</th><th>pulocationid</th><th>dolocationid</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>2</td><td>2022-01-01 18:04:06</td><td>2022-01-01 18:40:52</td><td>1</td><td>9.7</td><td>1</td><td>N</td><td>138</td><td>48</td><td>1</td><td>34.5</td><td>0.5</td><td>0.5</td><td>9.22</td><td>6.55</td><td>0.3</td><td>55.32</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:17:55</td><td>2022-01-01 00:27:45</td><td>1</td><td>4.22</td><td>1</td><td>N</td><td>138</td><td>92</td><td>2</td><td>14.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>16.55</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:55:48</td><td>2022-01-01 01:14:24</td><td>1</td><td>6.67</td><td>1</td><td>N</td><td>138</td><td>229</td><td>1</td><td>21.0</td><td>0.5</td><td>0.5</td><td>15.0</td><td>0.0</td><td>0.3</td><td>41.05</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:10:27</td><td>2022-01-01 00:41:59</td><td>1</td><td>17.1</td><td>2</td><td>N</td><td>132</td><td>170</td><td>1</td><td>52.0</td><td>3.75</td><td>0.5</td><td>15.75</td><td>6.55</td><td>0.3</td><td>78.85</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:50:27</td><td>2022-01-01 00:59:28</td><td>1</td><td>2.66</td><td>1</td><td>N</td><td>132</td><td>10</td><td>2</td><td>10.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>12.55</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:31:06</td><td>2022-01-01 00:58:26</td><td>3</td><td>19.14</td><td>2</td><td>N</td><td>132</td><td>263</td><td>1</td><td>52.0</td><td>0.0</td><td>0.5</td><td>12.37</td><td>6.55</td><td>0.3</td><td>75.47</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:42:45</td><td>2022-01-01 00:56:15</td><td>2</td><td>6.49</td><td>1</td><td>N</td><td>138</td><td>112</td><td>1</td><td>19.5</td><td>0.5</td><td>0.5</td><td>6.62</td><td>0.0</td><td>0.3</td><td>28.67</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:56:26</td><td>2022-01-01 01:25:09</td><td>1</td><td>18.81</td><td>2</td><td>N</td><td>132</td><td>148</td><td>1</td><td>52.0</td><td>0.0</td><td>0.5</td><td>11.31</td><td>0.0</td><td>0.3</td><td>67.86</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:33:42</td><td>2022-01-01 01:01:49</td><td>1</td><td>19.5</td><td>1</td><td>N</td><td>132</td><td>47</td><td>1</td><td>52.0</td><td>1.75</td><td>0.5</td><td>0.0</td><td>6.55</td><td>0.3</td><td>61.1</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:34:34</td><td>2022-01-01 01:08:45</td><td>2</td><td>20.7</td><td>2</td><td>N</td><td>132</td><td>236</td><td>1</td><td>52.0</td><td>3.75</td><td>0.5</td><td>11.3</td><td>0.0</td><td>0.3</td><td>67.85</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:05:57</td><td>2022-01-01 00:32:31</td><td>1</td><td>11.1</td><td>1</td><td>N</td><td>132</td><td>61</td><td>1</td><td>33.0</td><td>1.75</td><td>0.5</td><td>7.1</td><td>0.0</td><td>0.3</td><td>42.65</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:58:27</td><td>2022-01-01 01:23:12</td><td>1</td><td>16.75</td><td>1</td><td>N</td><td>132</td><td>112</td><td>1</td><td>45.5</td><td>0.5</td><td>0.5</td><td>9.61</td><td>0.0</td><td>0.3</td><td>57.66</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:25:46</td><td>2022-01-01 00:40:39</td><td>1</td><td>6.32</td><td>1</td><td>N</td><td>132</td><td>139</td><td>2</td><td>20.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>22.55</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:58:47</td><td>2022-01-01 01:24:38</td><td>1</td><td>16.82</td><td>1</td><td>N</td><td>132</td><td>255</td><td>2</td><td>46.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>48.55</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:48:52</td><td>2022-01-01 01:01:55</td><td>2</td><td>5.3</td><td>1</td><td>N</td><td>132</td><td>180</td><td>1</td><td>17.0</td><td>1.75</td><td>0.5</td><td>5.85</td><td>0.0</td><td>0.3</td><td>25.4</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:42:02</td><td>2022-01-01 01:04:31</td><td>1</td><td>14.48</td><td>1</td><td>N</td><td>132</td><td>7</td><td>1</td><td>40.0</td><td>0.5</td><td>0.5</td><td>8.51</td><td>0.0</td><td>0.3</td><td>51.06</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:30:25</td><td>2022-01-01 01:15:31</td><td>2</td><td>20.9</td><td>2</td><td>N</td><td>132</td><td>143</td><td>1</td><td>52.0</td><td>0.0</td><td>0.5</td><td>12.62</td><td>6.55</td><td>0.3</td><td>75.72</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:48:29</td><td>2022-01-01 01:17:18</td><td>1</td><td>18.9</td><td>2</td><td>N</td><td>132</td><td>231</td><td>1</td><td>52.0</td><td>3.75</td><td>0.5</td><td>11.3</td><td>0.0</td><td>0.3</td><td>67.85</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:26:37</td><td>2022-01-01 00:39:35</td><td>1</td><td>7.38</td><td>1</td><td>N</td><td>132</td><td>265</td><td>2</td><td>21.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>23.55</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>2</td><td>2022-01-01 00:28:11</td><td>2022-01-01 00:55:02</td><td>2</td><td>19.14</td><td>1</td><td>N</td><td>132</td><td>65</td><td>2</td><td>51.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>54.05</td><td>0.0</td><td>1.25</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|vendorid|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|ratecodeid|store_and_fwd_flag|pulocationid|dolocationid|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       2| 2022-01-01 18:04:06|  2022-01-01 18:40:52|              1|          9.7|         1|                 N|         138|          48|           1|       34.5|  0.5|    0.5|      9.22|        6.55|                  0.3|       55.32|                 2.5|       1.25|\n",
       "|       2| 2022-01-01 00:17:55|  2022-01-01 00:27:45|              1|         4.22|         1|                 N|         138|          92|           2|       14.0|  0.5|    0.5|       0.0|         0.0|                  0.3|       16.55|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:55:48|  2022-01-01 01:14:24|              1|         6.67|         1|                 N|         138|         229|           1|       21.0|  0.5|    0.5|      15.0|         0.0|                  0.3|       41.05|                 2.5|       1.25|\n",
       "|       1| 2022-01-01 00:10:27|  2022-01-01 00:41:59|              1|         17.1|         2|                 N|         132|         170|           1|       52.0| 3.75|    0.5|     15.75|        6.55|                  0.3|       78.85|                 2.5|       1.25|\n",
       "|       2| 2022-01-01 00:50:27|  2022-01-01 00:59:28|              1|         2.66|         1|                 N|         132|          10|           2|       10.0|  0.5|    0.5|       0.0|         0.0|                  0.3|       12.55|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:31:06|  2022-01-01 00:58:26|              3|        19.14|         2|                 N|         132|         263|           1|       52.0|  0.0|    0.5|     12.37|        6.55|                  0.3|       75.47|                 2.5|       1.25|\n",
       "|       2| 2022-01-01 00:42:45|  2022-01-01 00:56:15|              2|         6.49|         1|                 N|         138|         112|           1|       19.5|  0.5|    0.5|      6.62|         0.0|                  0.3|       28.67|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:56:26|  2022-01-01 01:25:09|              1|        18.81|         2|                 N|         132|         148|           1|       52.0|  0.0|    0.5|     11.31|         0.0|                  0.3|       67.86|                 2.5|       1.25|\n",
       "|       1| 2022-01-01 00:33:42|  2022-01-01 01:01:49|              1|         19.5|         1|                 N|         132|          47|           1|       52.0| 1.75|    0.5|       0.0|        6.55|                  0.3|        61.1|                 0.0|       1.25|\n",
       "|       1| 2022-01-01 00:34:34|  2022-01-01 01:08:45|              2|         20.7|         2|                 N|         132|         236|           1|       52.0| 3.75|    0.5|      11.3|         0.0|                  0.3|       67.85|                 2.5|       1.25|\n",
       "|       1| 2022-01-01 00:05:57|  2022-01-01 00:32:31|              1|         11.1|         1|                 N|         132|          61|           1|       33.0| 1.75|    0.5|       7.1|         0.0|                  0.3|       42.65|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:58:27|  2022-01-01 01:23:12|              1|        16.75|         1|                 N|         132|         112|           1|       45.5|  0.5|    0.5|      9.61|         0.0|                  0.3|       57.66|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:25:46|  2022-01-01 00:40:39|              1|         6.32|         1|                 N|         132|         139|           2|       20.0|  0.5|    0.5|       0.0|         0.0|                  0.3|       22.55|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:58:47|  2022-01-01 01:24:38|              1|        16.82|         1|                 N|         132|         255|           2|       46.0|  0.5|    0.5|       0.0|         0.0|                  0.3|       48.55|                 0.0|       1.25|\n",
       "|       1| 2022-01-01 00:48:52|  2022-01-01 01:01:55|              2|          5.3|         1|                 N|         132|         180|           1|       17.0| 1.75|    0.5|      5.85|         0.0|                  0.3|        25.4|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:42:02|  2022-01-01 01:04:31|              1|        14.48|         1|                 N|         132|           7|           1|       40.0|  0.5|    0.5|      8.51|         0.0|                  0.3|       51.06|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:30:25|  2022-01-01 01:15:31|              2|         20.9|         2|                 N|         132|         143|           1|       52.0|  0.0|    0.5|     12.62|        6.55|                  0.3|       75.72|                 2.5|       1.25|\n",
       "|       1| 2022-01-01 00:48:29|  2022-01-01 01:17:18|              1|         18.9|         2|                 N|         132|         231|           1|       52.0| 3.75|    0.5|      11.3|         0.0|                  0.3|       67.85|                 2.5|       1.25|\n",
       "|       2| 2022-01-01 00:26:37|  2022-01-01 00:39:35|              1|         7.38|         1|                 N|         132|         265|           2|       21.0|  0.5|    0.5|       0.0|         0.0|                  0.3|       23.55|                 0.0|       1.25|\n",
       "|       2| 2022-01-01 00:28:11|  2022-01-01 00:55:02|              2|        19.14|         1|                 N|         132|          65|           2|       51.5|  0.5|    0.5|       0.0|         0.0|                  0.3|       54.05|                 0.0|       1.25|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test filters here\n",
    "#sdf_yellow_2022_01.schema\n",
    "\n",
    "payment_cols = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
    "\n",
    "print(\"Total trips: \")\n",
    "\n",
    "print(sdf_yellow_2022_01.count())\n",
    "\n",
    "print(\"Credit card payment trips with valid total: \")\n",
    "\n",
    "print(sdf_yellow_2022_01.where((F.col('payment_type') == 1) & (F.col('total_amount') == sum(sdf_yellow_2022_01[col] for col in payment_cols))).count())\n",
    "\n",
    "print(\"Trips with valid total: \")\n",
    "\n",
    "print(sdf_yellow_2022_01.where(F.col('total_amount') == sum(sdf_yellow_2022_01[col] for col in payment_cols)).count())\n",
    "\n",
    "print(\"Cash payment trips with valid total: \")\n",
    "\n",
    "print(sdf_yellow_2022_01.where((F.col('payment_type') == 2) & (F.col('total_amount') == sum(sdf_yellow_2022_01[col] for col in payment_cols))).count())\n",
    "\n",
    "sdf_yellow_2022_01.where(\n",
    "    (F.col('payment_type') == 1) \n",
    "    & (F.col('total_amount') == sum(sdf_yellow_2022_01[col] for col in payment_cols))\n",
    "    & (F.col('tolls_amount') > 0)\n",
    "    & (F.col('airport_fee') > 0)\n",
    "    ).show(5)\n",
    "\n",
    "#sdf_yellow_2022_01.filter(F.col('passenger_count') == 5).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|vendorid|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|ratecodeid|store_and_fwd_flag|pulocationid|dolocationid|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>vendorid</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>ratecodeid</th><th>store_and_fwd_flag</th><th>pulocationid</th><th>dolocationid</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:35:40</td><td>2022-01-01 00:53:29</td><td>2</td><td>3.8</td><td>1</td><td>N</td><td>142</td><td>236</td><td>1</td><td>14.5</td><td>3.0</td><td>0.5</td><td>3.65</td><td>0.0</td><td>0.3</td><td>21.95</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:33:43</td><td>2022-01-01 00:42:07</td><td>1</td><td>2.1</td><td>1</td><td>N</td><td>236</td><td>42</td><td>1</td><td>8.0</td><td>0.5</td><td>0.5</td><td>4.0</td><td>0.0</td><td>0.3</td><td>13.3</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:40:15</td><td>2022-01-01 01:09:48</td><td>1</td><td>10.3</td><td>1</td><td>N</td><td>138</td><td>161</td><td>1</td><td>33.0</td><td>3.0</td><td>0.5</td><td>13.0</td><td>6.55</td><td>0.3</td><td>56.35</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:33:52</td><td>2022-01-01 00:47:28</td><td>3</td><td>4.2</td><td>1</td><td>N</td><td>148</td><td>141</td><td>1</td><td>14.0</td><td>2.5</td><td>0.5</td><td>3.45</td><td>0.0</td><td>0.3</td><td>20.75</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:53:54</td><td>2022-01-01 01:05:20</td><td>2</td><td>2.2</td><td>1</td><td>N</td><td>237</td><td>107</td><td>1</td><td>9.5</td><td>2.5</td><td>0.5</td><td>2.55</td><td>0.0</td><td>0.3</td><td>15.35</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:00:44</td><td>2022-01-01 00:05:29</td><td>1</td><td>0.2</td><td>1</td><td>N</td><td>7</td><td>7</td><td>2</td><td>5.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>6.3</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:35:50</td><td>2022-01-01 00:48:33</td><td>2</td><td>3.9</td><td>1</td><td>N</td><td>107</td><td>263</td><td>1</td><td>13.0</td><td>3.0</td><td>0.5</td><td>3.35</td><td>0.0</td><td>0.3</td><td>20.15</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:49:14</td><td>2022-01-01 00:58:29</td><td>2</td><td>3.2</td><td>1</td><td>N</td><td>263</td><td>107</td><td>1</td><td>11.0</td><td>3.0</td><td>0.5</td><td>2.95</td><td>0.0</td><td>0.3</td><td>17.75</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:06:10</td><td>2022-01-01 00:08:58</td><td>1</td><td>0.0</td><td>1</td><td>N</td><td>161</td><td>161</td><td>4</td><td>2.5</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>6.3</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:09:01</td><td>2022-01-01 00:17:15</td><td>1</td><td>1.2</td><td>1</td><td>N</td><td>161</td><td>43</td><td>1</td><td>7.0</td><td>3.0</td><td>0.5</td><td>2.15</td><td>0.0</td><td>0.3</td><td>12.95</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:30:06</td><td>2022-01-01 00:38:11</td><td>1</td><td>1.7</td><td>1</td><td>N</td><td>239</td><td>24</td><td>1</td><td>7.0</td><td>3.0</td><td>0.5</td><td>3.25</td><td>0.0</td><td>0.3</td><td>14.05</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:46:41</td><td>2022-01-01 00:57:53</td><td>1</td><td>1.6</td><td>1</td><td>N</td><td>239</td><td>263</td><td>1</td><td>8.0</td><td>3.0</td><td>0.5</td><td>2.35</td><td>0.0</td><td>0.3</td><td>14.15</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:52:53</td><td>2022-01-01 01:05:26</td><td>2</td><td>4.0</td><td>1</td><td>N</td><td>170</td><td>238</td><td>2</td><td>13.0</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>16.8</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:23:54</td><td>2022-01-01 00:28:42</td><td>2</td><td>0.8</td><td>1</td><td>N</td><td>237</td><td>237</td><td>4</td><td>5.5</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>9.3</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:59:06</td><td>2022-01-01 01:12:57</td><td>2</td><td>3.8</td><td>1</td><td>Y</td><td>233</td><td>238</td><td>2</td><td>13.5</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>17.3</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:10:27</td><td>2022-01-01 00:41:59</td><td>1</td><td>17.1</td><td>2</td><td>N</td><td>132</td><td>170</td><td>1</td><td>52.0</td><td>3.75</td><td>0.5</td><td>15.75</td><td>6.55</td><td>0.3</td><td>78.85</td><td>2.5</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:45:45</td><td>2022-01-01 01:01:45</td><td>1</td><td>4.8</td><td>1</td><td>N</td><td>170</td><td>74</td><td>2</td><td>16.5</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>20.3</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:33:42</td><td>2022-01-01 01:01:49</td><td>1</td><td>19.5</td><td>1</td><td>N</td><td>132</td><td>47</td><td>1</td><td>52.0</td><td>1.75</td><td>0.5</td><td>0.0</td><td>6.55</td><td>0.3</td><td>61.1</td><td>0.0</td><td>1.25</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:19:52</td><td>2022-01-01 00:26:57</td><td>2</td><td>0.4</td><td>1</td><td>N</td><td>48</td><td>48</td><td>2</td><td>6.0</td><td>3.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>9.8</td><td>2.5</td><td>0.0</td></tr>\n",
       "<tr><td>1</td><td>2022-01-01 00:30:20</td><td>2022-01-01 00:52:10</td><td>1</td><td>2.7</td><td>1</td><td>N</td><td>48</td><td>79</td><td>1</td><td>15.0</td><td>3.0</td><td>0.5</td><td>3.75</td><td>0.0</td><td>0.3</td><td>22.55</td><td>2.5</td><td>0.0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|vendorid|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|ratecodeid|store_and_fwd_flag|pulocationid|dolocationid|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "|       1| 2022-01-01 00:35:40|  2022-01-01 00:53:29|              2|          3.8|         1|                 N|         142|         236|           1|       14.5|  3.0|    0.5|      3.65|         0.0|                  0.3|       21.95|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:33:43|  2022-01-01 00:42:07|              1|          2.1|         1|                 N|         236|          42|           1|        8.0|  0.5|    0.5|       4.0|         0.0|                  0.3|        13.3|                 0.0|        0.0|\n",
       "|       1| 2022-01-01 00:40:15|  2022-01-01 01:09:48|              1|         10.3|         1|                 N|         138|         161|           1|       33.0|  3.0|    0.5|      13.0|        6.55|                  0.3|       56.35|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:33:52|  2022-01-01 00:47:28|              3|          4.2|         1|                 N|         148|         141|           1|       14.0|  2.5|    0.5|      3.45|         0.0|                  0.3|       20.75|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:53:54|  2022-01-01 01:05:20|              2|          2.2|         1|                 N|         237|         107|           1|        9.5|  2.5|    0.5|      2.55|         0.0|                  0.3|       15.35|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:00:44|  2022-01-01 00:05:29|              1|          0.2|         1|                 N|           7|           7|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 0.0|        0.0|\n",
       "|       1| 2022-01-01 00:35:50|  2022-01-01 00:48:33|              2|          3.9|         1|                 N|         107|         263|           1|       13.0|  3.0|    0.5|      3.35|         0.0|                  0.3|       20.15|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:49:14|  2022-01-01 00:58:29|              2|          3.2|         1|                 N|         263|         107|           1|       11.0|  3.0|    0.5|      2.95|         0.0|                  0.3|       17.75|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:06:10|  2022-01-01 00:08:58|              1|          0.0|         1|                 N|         161|         161|           4|        2.5|  3.0|    0.5|       0.0|         0.0|                  0.3|         6.3|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:09:01|  2022-01-01 00:17:15|              1|          1.2|         1|                 N|         161|          43|           1|        7.0|  3.0|    0.5|      2.15|         0.0|                  0.3|       12.95|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:30:06|  2022-01-01 00:38:11|              1|          1.7|         1|                 N|         239|          24|           1|        7.0|  3.0|    0.5|      3.25|         0.0|                  0.3|       14.05|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:46:41|  2022-01-01 00:57:53|              1|          1.6|         1|                 N|         239|         263|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:52:53|  2022-01-01 01:05:26|              2|          4.0|         1|                 N|         170|         238|           2|       13.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        16.8|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:23:54|  2022-01-01 00:28:42|              2|          0.8|         1|                 N|         237|         237|           4|        5.5|  3.0|    0.5|       0.0|         0.0|                  0.3|         9.3|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:59:06|  2022-01-01 01:12:57|              2|          3.8|         1|                 Y|         233|         238|           2|       13.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        17.3|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:10:27|  2022-01-01 00:41:59|              1|         17.1|         2|                 N|         132|         170|           1|       52.0| 3.75|    0.5|     15.75|        6.55|                  0.3|       78.85|                 2.5|       1.25|\n",
       "|       1| 2022-01-01 00:45:45|  2022-01-01 01:01:45|              1|          4.8|         1|                 N|         170|          74|           2|       16.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        20.3|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:33:42|  2022-01-01 01:01:49|              1|         19.5|         1|                 N|         132|          47|           1|       52.0| 1.75|    0.5|       0.0|        6.55|                  0.3|        61.1|                 0.0|       1.25|\n",
       "|       1| 2022-01-01 00:19:52|  2022-01-01 00:26:57|              2|          0.4|         1|                 N|          48|          48|           2|        6.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|        0.0|\n",
       "|       1| 2022-01-01 00:30:20|  2022-01-01 00:52:10|              1|          2.7|         1|                 N|          48|          79|           1|       15.0|  3.0|    0.5|      3.75|         0.0|                  0.3|       22.55|                 2.5|        0.0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_yellow_2022_01.where(\n",
    "    (F.col('airport_fee') == 0)\n",
    "    & (F.col('pulocationid') != 132)\n",
    "    & (F.col('pulocationid') != 138)\n",
    "    | (F.col('pulocationid') == 132)\n",
    "    & (F.col('airport_fee') == 1.25)\n",
    "    | (F.col('pulocationid') == 138)\n",
    "    & (F.col('airport_fee') == 1.25)\n",
    "    ).where(\n",
    "        (F.col('airport_fee') != 1.25)\n",
    "        & (F.col('pulocationid') == 132)\n",
    "        & (F.col('pulocationid') == 138)\n",
    "        ).show(5)\n",
    "\n",
    "'''\n",
    "sdf_yellow_2022_01.withColumn(\n",
    "    'is_valid_record',\n",
    "    # when we have a positive distance/passenger/total amount then True\n",
    "    # else False\n",
    "    F.when(\n",
    "        (F.col('airport_fee') == 0)\n",
    "        & (F.col('pulocationid') != 132)\n",
    "        & (F.col('pulocationid') != 138)\n",
    "        | (F.col('pulocationid') == 132)\n",
    "        & (F.col('airport_fee') == 1.25)\n",
    "        | (F.col('pulocationid') == 138)\n",
    "        & (F.col('airport_fee') == 1.25),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ").where(F.col('airport_fee') == 1.25)\n",
    "'''\n",
    "sdf_yellow_2022_01.where(\n",
    "    (F.col('vendorid') == 1)\n",
    "    | (F.col('vendorid') == 2)\n",
    "    ).where(\n",
    "        (F.col('vendorid') == 1)\n",
    "        & (F.col('vendorid') != 2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use business logic to filter and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m sdf_cleaned \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00myellow_cleaned_dir\u001b[39m}\u001b[39;00m\u001b[39m2022/2022-\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(month)\u001b[39m.\u001b[39mzfill(\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m\"\u001b[39m)  \n\u001b[1;32m     11\u001b[0m \u001b[39m# remove invalid records by applying business logic\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sdf_cleaned \u001b[39m=\u001b[39m \\\n\u001b[1;32m     13\u001b[0m sdf_cleaned\u001b[39m.\u001b[39mwhere(\n\u001b[0;32m---> 14\u001b[0m     (F\u001b[39m.\u001b[39;49myear(F\u001b[39m.\u001b[39;49mcol(\u001b[39m'\u001b[39;49m\u001b[39mtpep_pickup_datetime\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39m==\u001b[39;49m year) \u001b[39m# ensure that this trip was initiated within the correct year (assuming a trip belongs to a month based on pickup time)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m     (F\u001b[39m.\u001b[39;49mmonth(F\u001b[39m.\u001b[39;49mcol(\u001b[39m'\u001b[39;49m\u001b[39mtpep_pickup_datetime\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39m==\u001b[39;49m month)  \u001b[39m# ensure that this trip was initiated within the correct month (assuming a trip belongs to a month based on pickup time)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39munix_timestamp(\u001b[39m'\u001b[39m\u001b[39mtpep_pickup_datetime\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m<\u001b[39m F\u001b[39m.\u001b[39munix_timestamp(\u001b[39m'\u001b[39m\u001b[39mtpep_dropoff_datetime\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m# make sure pickup time is earlier than dropoff time\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mpassenger_count\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m# ensure non-zero passenger count, trip distance and total amount\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtrip_distance\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \n\u001b[1;32m     20\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \n\u001b[1;32m     21\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \n\u001b[1;32m     22\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \n\u001b[1;32m     23\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \n\u001b[1;32m     24\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \n\u001b[1;32m     25\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mpayment_type\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m# include only credit card payments as cash tips are not included\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m'\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39msum\u001b[39m(sdf_yellow_2022_01[col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m payment_cols)) \u001b[39m# check the total amount equals the sum of all other fees\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \n\u001b[1;32m     28\u001b[0m )\u001b[39m.\u001b[39mwithColumn(\n\u001b[1;32m     29\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstore_and_fwd_flag\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m     (F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mstore_and_fwd_flag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mY\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m'\u001b[39m\u001b[39mBOOLEAN\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m sdf_cleaned \u001b[39m=\u001b[39m sdf_cleaned\u001b[39m.\u001b[39mwithColumn(\n\u001b[1;32m     34\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstore_and_fwd_flag\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m     (F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mstore_and_fwd_flag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mY\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m'\u001b[39m\u001b[39mBOOLEAN\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     44\u001b[0m sdf_cleaned \\\n\u001b[1;32m     45\u001b[0m \u001b[39m.\u001b[39mcoalesce(\u001b[39m1\u001b[39m) \\\n\u001b[1;32m     46\u001b[0m \u001b[39m.\u001b[39mwrite \\\n\u001b[1;32m     47\u001b[0m \u001b[39m.\u001b[39mmode(\u001b[39m'\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m     48\u001b[0m \u001b[39m.\u001b[39mparquet(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00myellow_curated_dir\u001b[39m}\u001b[39;00m\u001b[39m2022/2022-\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(month)\u001b[39m.\u001b[39mzfill(\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "payment_cols = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 2022\n",
    "for month in range(1,13):\n",
    "\n",
    "    year = 2022\n",
    "    sdf_cleaned = spark.read.parquet(f\"{yellow_cleaned_dir}2022/2022-{str(month).zfill(2)}.parquet\")  \n",
    "\n",
    "    #laguardia_id = \n",
    "    #jfk_id = \n",
    "    \n",
    "    '''\n",
    "    sdf = sdf.withColumn(\n",
    "        'is_valid_record',\n",
    "        # when we have a positive distance/passenger/total amount then True\n",
    "        # else False\n",
    "        F.when(\n",
    "            (F.col('trip_distance') > 0)\n",
    "            & (F.col('passenger_count') > 0)\n",
    "            & (F.col('total_amount') > 0),\n",
    "            True\n",
    "        ).otherwise(False)\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    # may have to go through data and apply invalid record boolean attribute first for more complex business logic checking such as airport fee matching pickup at laguardia / jfk\n",
    "    \n",
    "    # remove invalid records by applying business logic\n",
    "    sdf_cleaned = \\\n",
    "    sdf_cleaned.where(\n",
    "        (F.year(F.col('tpep_pickup_datetime')) == year) # ensure that this trip was initiated within the correct year (assuming a trip belongs to a year based on pickup time)\n",
    "        (F.month(F.col('tpep_pickup_datetime')) == month)  # ensure that this trip was initiated within the correct month (assuming a trip belongs to a month based on pickup time)\n",
    "        & (F.unix_timestamp('tpep_pickup_datetime') < F.unix_timestamp('tpep_dropoff_datetime')) # make sure pickup time is earlier than dropoff time\n",
    "        & (F.col('passenger_count') > 0) # ensure non-zero passenger count, trip distance and total amount\n",
    "        & (F.col('trip_distance') > 0)\n",
    "        & (F.col('total_amount') > 0) \n",
    "        & (F.col('fare_amount') >= 0) # ensure non-negative fees\n",
    "        & (F.col('extra') >= 0) \n",
    "        & (F.col('mta_tax') >= 0) \n",
    "        & (F.col('tip_amount') >= 0) \n",
    "        & (F.col('tolls_amount') >= 0)\n",
    "        & (F.col('improvement_surcharge') >= 0) \n",
    "        & (F.col('congestion_surcharge') >= 0)  \n",
    "        & (F.col('payment_type') == 1) # include only credit card payments as cash tips are not included in this data\n",
    "        & (F.col('total_amount') == sum(sdf_yellow_2022_01[col] for col in payment_cols)) # check the total amount equals the sum of all other fees\n",
    "    ).where(\n",
    "        (F.col('vendorid') == 1)\n",
    "        | (F.col('vendorid') == 2)  \n",
    "    ).where(\n",
    "        (F.col('airport_fee') == 0)\n",
    "        & (F.col('pulocationid') != 132)\n",
    "        & (F.col('pulocationid') != 138)\n",
    "        | (F.col('pulocationid') == 132)\n",
    "        & (F.col('airport_fee') == 1.25)\n",
    "        | (F.col('pulocationid') == 138)\n",
    "        & (F.col('airport_fee') == 1.25)  \n",
    "    ).withColumn(\n",
    "        'store_and_fwd_flag',\n",
    "        (F.col(\"store_and_fwd_flag\") == 'Y').cast('BOOLEAN')\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    sdf_cleaned = sdf_cleaned.withColumn(\n",
    "        'store_and_fwd_flag',\n",
    "        (F.col(\"store_and_fwd_flag\") == 'Y').cast('BOOLEAN')\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    sdf_cleaned \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{yellow_curated_dir}2022/2022-{str(month).zfill(2)}.parquet\")\n",
    "\n",
    "# 2023\n",
    "for month in range(1,5):\n",
    "\n",
    "    year = 2023\n",
    "    sdf_cleaned = spark.read.parquet(f\"{yellow_cleaned_dir}2023/2023-{str(month).zfill(2)}.parquet\")\n",
    "\n",
    "    # select all columns from the existing cleaned dataframe and cast it to the required schema\n",
    "    sdf_cleaned = sdf_cleaned \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_cleaned.columns)])\n",
    "     \n",
    "    # apply business logic\n",
    "    \n",
    "    sdf_cleaned = sdf_cleaned.withColumn(\n",
    "        'store_and_fwd_flag',\n",
    "        (F.col(\"store_and_fwd_flag\") == 'Y').cast('BOOLEAN')\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    sdf_cleaned \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{yellow_curated_dir}2023/2023-{str(month).zfill(2)}.parquet\")\n",
    "    \n",
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check curated 2022 data\n",
    "\n",
    "sdf1 = spark.read.schema(sdf_schema).parquet('../data/cleaned/tlc/yellow/2022/*')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check curated 2023 data\n",
    "\n",
    "sdf = spark.read.schema(sdf_schema).parquet('../data/cleaned/tlc/yellow/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Green Taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for cleaned data\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/cleaned/'):\n",
    "    os.makedirs('../data/cleaned/')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc'):\n",
    "    os.makedirs('../data/cleaned/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/green'):\n",
    "    os.makedirs('../data/cleaned/tlc/green')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/green/2022'):\n",
    "    os.makedirs('../data/cleaned/tlc/green/2022')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/green/2023'):\n",
    "    os.makedirs('../data/cleaned/tlc/green/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_raw_dir = '../data/raw/tlc/green/'\n",
    "\n",
    "sdf_green_2022_01 = spark.read.parquet(f\"{green_raw_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_green_2022_all = spark.read.parquet(f\"{green_raw_dir}2022\")\n",
    "\n",
    "sdf_green_2023_01 = spark.read.parquet(f\"{green_raw_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_green_2023_02 = spark.read.parquet(f\"{green_raw_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_green_2022_all = spark.read.parquet('../data/raw/green/2022')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2022_all.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2023_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', LongType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_green_2023_02.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas appear to be incorrect for all of 2022, so we will adjust all datasets to line up with the schema of Feb 2023 data and ensure consistent lowercase casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('ratecodeid', IntegerType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''\n",
    "\n",
    "# RatecodeID and passenger_count also need to be converted to int\n",
    "\n",
    "sdf_green_2023_02 = sdf_green_2023_02.withColumn(\n",
    "    'ratecodeid', \n",
    "    F.col('RatecodeID').cast('Integer') \n",
    ")\n",
    "\n",
    "sdf_green_2023_02 = sdf_green_2023_02.withColumn(\n",
    "    'passenger_count',\n",
    "    F.col('passenger_count').cast('Integer')\n",
    ")\n",
    "\n",
    "\n",
    "sdf_green_2023_02.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('vendorid', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampNTZType(), True), StructField('lpep_dropoff_datetime', TimestampNTZType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('ratecodeid', IntegerType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we want to ensure everything has consistent casing to make our lives easier\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_green_2023_02.columns]\n",
    "sdf_green_2023_02 = sdf_green_2023_02.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_green_2023_02.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all raw datasets to required schema\n",
    "\n",
    "green_raw_dir = '../data/raw/tlc/green/'\n",
    "    \n",
    "green_clean_dir = '../data/cleaned/tlc/green/'\n",
    "\n",
    "# 2022\n",
    "for month in range(1,13):\n",
    "\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{green_raw_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)])\n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{green_clean_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "# 2023\n",
    "for month in range(1,5):\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{green_raw_dir}2023/2023-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)])\n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{green_clean_dir}2023/2023-{month}.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 1                   \n",
      " lpep_pickup_datetime  | 2022-03-01 00:24:14 \n",
      " lpep_dropoff_datetime | 2022-03-01 00:34:04 \n",
      " store_and_fwd_flag    | N                   \n",
      " ratecodeid            | 1                   \n",
      " pulocationid          | 74                  \n",
      " dolocationid          | 151                 \n",
      " passenger_count       | 1                   \n",
      " trip_distance         | 2.3                 \n",
      " fare_amount           | 9.5                 \n",
      " extra                 | 0.5                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 0.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " ehail_fee             | null                \n",
      " improvement_surcharge | 0.3                 \n",
      " total_amount          | 10.8                \n",
      " payment_type          | 2                   \n",
      " trip_type             | 1                   \n",
      " congestion_surcharge  | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\\nsdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\\n\\nsdf2.show(1, vertical=True, truncate=100)\\n\\nprint(sdf1.count())\\n\\nprint(sdf2.count())\\n\\nsdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\\n\\nsdf.show(1, vertical=True, truncate=100)\\n\\nprint(sdf.count())\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check schema of 2022 cleaned data\n",
    "\n",
    "sdf1 = spark.read.schema(sdf_schema).parquet('../data/cleaned/tlc/green/2022/*')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " vendorid              | 2                   \n",
      " lpep_pickup_datetime  | 2023-03-01 00:25:10 \n",
      " lpep_dropoff_datetime | 2023-03-01 00:35:47 \n",
      " store_and_fwd_flag    | N                   \n",
      " ratecodeid            | 1                   \n",
      " pulocationid          | 82                  \n",
      " dolocationid          | 196                 \n",
      " passenger_count       | 1                   \n",
      " trip_distance         | 2.36                \n",
      " fare_amount           | 13.5                \n",
      " extra                 | 1.0                 \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 0.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " ehail_fee             | null                \n",
      " improvement_surcharge | 1.0                 \n",
      " total_amount          | 16.0                \n",
      " payment_type          | 2                   \n",
      " trip_type             | 1                   \n",
      " congestion_surcharge  | 0.0                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double check schema of 2023 cleaned data\n",
    "\n",
    "sdf = spark.read.schema(sdf_schema).parquet('../data/cleaned/tlc/green/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Volume For-Hire Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for cleaned data\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/cleaned/'):\n",
    "    os.makedirs('../data/cleaned/')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc'):\n",
    "    os.makedirs('../data/cleaned/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/fhvhv'):\n",
    "    os.makedirs('../data/cleaned/tlc/fhvhv')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/fhvhv/2022'):\n",
    "    os.makedirs('../data/cleaned/tlc/fhvhv/2022')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/fhvhv/2023'):\n",
    "    os.makedirs('../data/cleaned/tlc/fhvhv/2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_raw_dir = '../data/raw/tlc/fhvhv/'\n",
    "\n",
    "sdf_fhvhv_2022_01 = spark.read.parquet(f\"{fhvhv_raw_dir}2022/2022-01.parquet\")\n",
    "\n",
    "sdf_fhvhv_2022_all = spark.read.parquet(f\"{fhvhv_raw_dir}2022\")\n",
    "\n",
    "sdf_fhvhv_2023_01 = spark.read.parquet(f\"{fhvhv_raw_dir}2023/2023-01.parquet\")\n",
    "\n",
    "sdf_fhvhv_2023_02 = spark.read.parquet(f\"{fhvhv_raw_dir}2023/2023-02.parquet\")\n",
    "\n",
    "#sdf_fhvhv_2022_all = spark.read.parquet(f\"{fhvhv_raw_dir}2022\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2022_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2022_all.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2023_01.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhvhv_2023_02.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas appear to be incorrect for all of 2022, so we will adjust all datasets to line up with the schema of Feb 2023 data and ensure consistent lowercase casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', IntegerType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# RatecodeID and passenger_count also need to be converted to int\n",
    "'''\n",
    "# renaming\n",
    "sdf.withColumnRenamed(\n",
    "    'column_from',\n",
    "    'column_to'\n",
    ")\n",
    "\n",
    "# example 1 for converting data types\n",
    "sdf.withColumn(\n",
    "    'column_to',\n",
    "    F.col('column_from').cast('data type')\n",
    ")\n",
    "'''\n",
    "\n",
    "# trip_time needs to be converted to integer\n",
    "\n",
    "sdf_fhvhv_2023_02 = sdf_fhvhv_2023_02.withColumn(\n",
    "    'trip_time', \n",
    "    F.col('trip_time').cast('Integer') \n",
    ")\n",
    "\n",
    "sdf_fhvhv_2023_02.schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('pulocationid', IntegerType(), True), StructField('dolocationid', IntegerType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', IntegerType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we want to ensure everything has consistent casing to make our lives easier\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in sdf_fhvhv_2023_02.columns]\n",
    "sdf_fhvhv_2023_02 = sdf_fhvhv_2023_02.select(*consistent_col_casing)\n",
    "\n",
    "# this will be used in the cell below when reading in\n",
    "sdf_schema = sdf_fhvhv_2023_02.schema\n",
    "sdf_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert all raw datasets to required schema\n",
    "\n",
    "fhvhv_raw_dir = '../data/raw/tlc/fhvhv/'\n",
    "    \n",
    "fhvhv_clean_dir = '../data/cleaned/tlc/fhvhv/'\n",
    "\n",
    "# 2022\n",
    "for month in range(1,13):\n",
    "\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{fhvhv_raw_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)])\n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{fhvhv_clean_dir}2022/2022-{month}.parquet\")\n",
    "\n",
    "# 2023\n",
    "for month in range(1,5):\n",
    "    month = str(month).zfill(2) \n",
    "\n",
    "    sdf_malformed = spark.read.parquet(f\"{fhvhv_raw_dir}2023/2023-{month}.parquet\")\n",
    "\n",
    "    # select all columns from the existing malformed dataframe and cast it to the required schema\n",
    "    sdf_malformed = sdf_malformed \\\n",
    "        .select([F.col(c).cast(sdf_schema[i].dataType) for i, c in enumerate(sdf_malformed.columns)])\n",
    "        \n",
    "    sdf_malformed \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(f\"{fhvhv_clean_dir}2023/2023-{month}.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " hvfhs_license_num    | HV0003              \n",
      " dispatching_base_num | B03404              \n",
      " originating_base_num | B03404              \n",
      " request_datetime     | 2022-01-01 00:05:31 \n",
      " on_scene_datetime    | 2022-01-01 00:05:40 \n",
      " pickup_datetime      | 2022-01-01 00:07:24 \n",
      " dropoff_datetime     | 2022-01-01 00:18:28 \n",
      " pulocationid         | 170                 \n",
      " dolocationid         | 161                 \n",
      " trip_miles           | 1.18                \n",
      " trip_time            | 664                 \n",
      " base_passenger_fare  | 24.9                \n",
      " tolls                | 0.0                 \n",
      " bcf                  | 0.75                \n",
      " sales_tax            | 2.21                \n",
      " congestion_surcharge | 2.75                \n",
      " airport_fee          | 0.0                 \n",
      " tips                 | 0.0                 \n",
      " driver_pay           | 23.03               \n",
      " shared_request_flag  | N                   \n",
      " shared_match_flag    | N                   \n",
      " access_a_ride_flag   |                     \n",
      " wav_request_flag     | N                   \n",
      " wav_match_flag       | N                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\\nsdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\\n\\nsdf2.show(1, vertical=True, truncate=100)\\n\\nprint(sdf1.count())\\n\\nprint(sdf2.count())\\n\\nsdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\\n\\nsdf.show(1, vertical=True, truncate=100)\\n\\nprint(sdf.count())\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check schema of 2022 cleaned data\n",
    "\n",
    "sdf1 = spark.read.schema(sdf_schema).parquet('../data/cleaned/tlc/fhvhv/2022/*')\n",
    "\n",
    "sdf1.show(1, vertical=True, truncate=100)\n",
    "\n",
    "''' checking raw vs cleaned dataset shape and raw dataset shape of jan 2022\n",
    "sdf2 = spark.read.parquet('../data/raw/tlc/yellow/2022')\n",
    "\n",
    "sdf2.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf1.count())\n",
    "\n",
    "print(sdf2.count())\n",
    "\n",
    "sdf = spark.read.parquet('../data/raw/tlc/yellow/2022/2022-01.parquet')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)\n",
    "\n",
    "print(sdf.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " hvfhs_license_num    | HV0003              \n",
      " dispatching_base_num | B03404              \n",
      " originating_base_num | B03404              \n",
      " request_datetime     | 2023-01-01 00:18:06 \n",
      " on_scene_datetime    | 2023-01-01 00:19:24 \n",
      " pickup_datetime      | 2023-01-01 00:19:38 \n",
      " dropoff_datetime     | 2023-01-01 00:48:07 \n",
      " pulocationid         | 48                  \n",
      " dolocationid         | 68                  \n",
      " trip_miles           | 0.94                \n",
      " trip_time            | 1709                \n",
      " base_passenger_fare  | 25.95               \n",
      " tolls                | 0.0                 \n",
      " bcf                  | 0.78                \n",
      " sales_tax            | 2.3                 \n",
      " congestion_surcharge | 2.75                \n",
      " airport_fee          | 0.0                 \n",
      " tips                 | 5.22                \n",
      " driver_pay           | 27.83               \n",
      " shared_request_flag  | N                   \n",
      " shared_match_flag    | N                   \n",
      " access_a_ride_flag   |                     \n",
      " wav_request_flag     | N                   \n",
      " wav_match_flag       | N                   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double check schema of 2023 cleaned data\n",
    "\n",
    "sdf = spark.read.schema(sdf_schema).parquet('../data/cleaned/tlc/fhvhv/2023/*')\n",
    "\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories for cleaned data\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/cleaned/'):\n",
    "    os.makedirs('../data/cleaned/')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc'):\n",
    "    os.makedirs('../data/cleaned/tlc')\n",
    "    \n",
    "if not os.path.exists('../data/cleaned/tlc/taxi_zones'):\n",
    "    os.makedirs('../data/cleaned/tlc/taxi_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_lookup = spark.read.option(\"header\",True) \\\n",
    "                   .csv(\"../data/raw/tlc/taxi_zones/taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n",
      "StructType([StructField('LocationID', StringType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(taxi_zone_lookup.limit(5))\n",
    "\n",
    "print(taxi_zone_lookup.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just need to convert the column names to lowercase for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|locationid|      borough|                zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n",
      "StructType([StructField('locationid', StringType(), True), StructField('borough', StringType(), True), StructField('zone', StringType(), True), StructField('service_zone', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# converting to lower case\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in taxi_zone_lookup.columns]\n",
    "taxi_zone_lookup = taxi_zone_lookup.select(*consistent_col_casing)\n",
    "\n",
    "print(taxi_zone_lookup.limit(5))\n",
    "print(taxi_zone_lookup.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save cleaned data as parquet\n",
    "taxi_zone_lookup \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('../data/cleaned/tlc/taxi_zones/taxi_zone_lookup.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of +----------+-------------+--------------------+------------+\n",
       "|locationid|      borough|                zone|service_zone|\n",
       "+----------+-------------+--------------------+------------+\n",
       "|         1|          EWR|      Newark Airport|         EWR|\n",
       "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
       "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
       "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
       "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
       "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
       "|         7|       Queens|             Astoria|   Boro Zone|\n",
       "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
       "|         9|       Queens|          Auburndale|   Boro Zone|\n",
       "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
       "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
       "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
       "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
       "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
       "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
       "|        16|       Queens|             Bayside|   Boro Zone|\n",
       "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
       "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
       "|        19|       Queens|           Bellerose|   Boro Zone|\n",
       "|        20|        Bronx|             Belmont|   Boro Zone|\n",
       "+----------+-------------+--------------------+------------+\n",
       "only showing top 20 rows\n",
       ">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned data\n",
    "taxi_zone_lookup = spark.read.parquet('../data/cleaned/tlc/taxi_zones/taxi_zone_lookup.parquet')\n",
    "\n",
    "taxi_zone_lookup.printSchema\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
