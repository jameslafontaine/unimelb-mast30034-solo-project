{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval / Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "import os\n",
    "\n",
    "output_relative_dir = '../data/landing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLC Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = ('2021', '2022', '2023')\n",
    "\n",
    "MONTHS = range(1, 13)\n",
    "\n",
    "VEHICLE_TYPES = ('yellow', 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# navigate to the raw data directory\n",
    "output_relative_dir = '../data/landing/tlc/'\n",
    "\n",
    "# check if it exists as it makedir will raise an error if it does exist\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    \n",
    "# now, for each type of data set we will need, we will create the paths\n",
    "for type in VEHICLE_TYPES:\n",
    "    for year in YEARS:\n",
    "        if not os.path.exists(output_relative_dir + type + '/' + year):\n",
    "            os.makedirs(output_relative_dir + type + '/' + year)\n",
    "\n",
    "if not os.path.exists(output_relative_dir + 'taxi_zones'):\n",
    "    os.makedirs(output_relative_dir + 'taxi_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the URL template as of 07/2023\n",
    "URL_TEMPLATE = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"#vehicleType_tripdata_year-month.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin yellow\n",
      "Begin year 2021\n",
      "Begin month 12\n",
      "Completed month 12\n",
      "Completed year 2021\n",
      "Begin year 2022\n",
      "Begin month 01\n",
      "Completed month 01\n",
      "Begin month 02\n",
      "Completed month 02\n",
      "Begin month 03\n",
      "Completed month 03\n",
      "Begin month 04\n",
      "Completed month 04\n",
      "Begin month 05\n",
      "Completed month 05\n",
      "Begin month 06\n",
      "Completed month 06\n",
      "Begin month 07\n",
      "Completed month 07\n",
      "Begin month 08\n",
      "Completed month 08\n",
      "Begin month 09\n",
      "Completed month 09\n",
      "Begin month 10\n",
      "Completed month 10\n",
      "Begin month 11\n",
      "Completed month 11\n",
      "Begin month 12\n",
      "Completed month 12\n",
      "Completed year 2022\n",
      "Begin year 2023\n",
      "Begin month 01\n",
      "Completed month 01\n",
      "Begin month 02\n",
      "Completed month 02\n",
      "Begin month 03\n",
      "Completed month 03\n",
      "Begin month 04\n",
      "Completed month 04\n",
      "Completed year 2023\n",
      "Completed yellow\n",
      "##########################################################\n",
      "Begin green\n",
      "Begin year 2021\n",
      "Begin month 12\n",
      "Completed month 12\n",
      "Completed year 2021\n",
      "Begin year 2022\n",
      "Begin month 01\n",
      "Completed month 01\n",
      "Begin month 02\n",
      "Completed month 02\n",
      "Begin month 03\n",
      "Completed month 03\n",
      "Begin month 04\n",
      "Completed month 04\n",
      "Begin month 05\n",
      "Completed month 05\n",
      "Begin month 06\n",
      "Completed month 06\n",
      "Begin month 07\n",
      "Completed month 07\n",
      "Begin month 08\n",
      "Completed month 08\n",
      "Begin month 09\n",
      "Completed month 09\n",
      "Begin month 10\n",
      "Completed month 10\n",
      "Begin month 11\n",
      "Completed month 11\n",
      "Begin month 12\n",
      "Completed month 12\n",
      "Completed year 2022\n",
      "Begin year 2023\n",
      "Begin month 01\n",
      "Completed month 01\n",
      "Begin month 02\n",
      "Completed month 02\n",
      "Begin month 03\n",
      "Completed month 03\n",
      "Begin month 04\n",
      "Completed month 04\n",
      "Completed year 2023\n",
      "Completed green\n",
      "##########################################################\n"
     ]
    }
   ],
   "source": [
    "# data output relative directory is `../data/landing/`\n",
    "\n",
    "# download tlc datasets\n",
    "for type in VEHICLE_TYPES:\n",
    "    print(f\"Begin {type}\")\n",
    "    for year in YEARS:\n",
    "        tlc_output_dir = output_relative_dir + type + '/' + year\n",
    "        print(f\"Begin year {year}\")\n",
    "        for month in MONTHS:\n",
    "            if (year == '2023' and month > 4): # stop at april for 2023\n",
    "                break\n",
    "            if (year == '2021' and month != 12): # only grab december for 2021\n",
    "                continue\n",
    "            # 0-fill i.e 1 -> 01, 2 -> 02, etc\n",
    "            month = str(month).zfill(2) \n",
    "            print(f\"Begin month {month}\")\n",
    "            \n",
    "            # generate url\n",
    "            url = f'{URL_TEMPLATE}{type}_tripdata_{year}-{month}.parquet'\n",
    "\n",
    "            # generate output location and filename\n",
    "            output_dir = f\"{tlc_output_dir}/{year}-{month}.parquet\"\n",
    "            # download\n",
    "            urlretrieve(url, output_dir) \n",
    "            \n",
    "            print(f\"Completed month {month}\")\n",
    "        print(f\"Completed year {year}\")\n",
    "    print(f\"Completed {type}\")\n",
    "    print(f\"##########################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download taxi zones csv and shapefile\n",
    "\n",
    "import zipfile\n",
    "\n",
    "output_dir = f\"{output_relative_dir}/taxi_zones/taxi+_zone_lookup.csv\"\n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv'\n",
    "\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "output_dir = f\"{output_relative_dir}/taxi_zones/taxi_zones.zip\"\n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip'\n",
    "\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "# unzip shapefile\n",
    "with zipfile.ZipFile(output_dir, 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"{output_relative_dir}/taxi_zones/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motor Vehicle Collisions NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/landing/mv_collisions/mv_collisions.csv',\n",
       " <http.client.HTTPMessage at 0x7fb22822ce50>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../data/landing/mv_collisions'):\n",
    "    os.makedirs('../data/landing/mv_collisions')\n",
    "\n",
    "url = 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "output_dir = '../data/landing/mv_collisions/mv_collisions.csv'\n",
    "\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(output_dir, dtype={'ZIP CODE': object}).to_parquet('../data/landing/mv_collisions/mv_collisions.parquet')\n",
    "\n",
    "#mv_collisions = pd.read_csv(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing mv_collisions parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_collisions = spark.read.parquet('../data/landing/mv_collisions/mv_collisions.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013583"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_collisions.schema\n",
    "\n",
    "mv_collisions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit Subway Entrances and Exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/landing/subway_entr_exit/subway_entr_exit.csv',\n",
       " <http.client.HTTPMessage at 0x7fb2285f4eb0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if not os.path.exists(output_relative_dir + 'subway_entr_exit'):\n",
    "    os.makedirs(output_relative_dir + 'subway_entr_exit')\n",
    "\n",
    "url = 'https://data.ny.gov/resource/i9wp-a4ja.csv?$query=SELECT%0A%20%20median(%60division%60)%20AS%20%60division%60%2C%0A%20%20median(%60line%60)%20AS%20%60line%60%2C%0A%20%20%60station_location%60%0AGROUP%20BY%20%60station_location%60'\n",
    "\n",
    "output_dir = '../data/landing/subway_entr_exit/subway_entr_exit.csv'\n",
    "\n",
    "urlretrieve(url, output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(output_dir).to_parquet('../data/landing/subway_entr_exit/subway_entr_exit.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing subway_entr_exit parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway_entr_exit = spark.read.parquet('../data/landing/subway_entr_exit/subway_entr_exit.parquet')\n",
    "\n",
    "\n",
    "subway_entr_exit.limit(5)\n",
    "\n",
    "subway_entr_exit.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit Subway Hourly Ridership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/landing/subway_hourly/subway_hourly.csv',\n",
       " <http.client.HTTPMessage at 0x7f7045dc8550>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if not os.path.exists(output_relative_dir + 'subway_hourly'):\n",
    "    os.makedirs(output_relative_dir + 'subway_hourly')\n",
    "    \n",
    "url = 'https://data.ny.gov/api/geospatial/wujg-7c2s?accessType=DOWNLOAD&method=export&format=GeoJSON'\n",
    "\n",
    "output_dir = '../data/landing/subway_hourly/subway_hourly.geojson'\n",
    "\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "url = 'https://data.ny.gov/api/views/wujg-7c2s/rows.csv?accessType=DOWNLOAD&sorting=true'\n",
    "\n",
    "output_dir = '../data/landing/subway_hourly/subway_hourly.csv'\n",
    "\n",
    "urlretrieve(url, output_dir) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(output_dir).to_parquet('../data/landing/subway_hourly/subway_hourly.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing subway_hourly parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5364324"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway_hourly = spark.read.parquet('../data/landing/subway_hourly/subway_hourly.parquet')\n",
    "\n",
    "\n",
    "subway_hourly.limit(5)\n",
    "\n",
    "subway_hourly.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit Bus Hourly Ridership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/landing/bus_hourly/bus_hourly.csv',\n",
       " <http.client.HTTPMessage at 0x7fb22822e440>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if not os.path.exists(output_relative_dir + 'bus_hourly'):\n",
    "    os.makedirs(output_relative_dir + 'bus_hourly')\n",
    "\n",
    "url = 'https://data.ny.gov/api/views/kv7t-n8in/rows.csv?accessType=DOWNLOAD&sorting=true'\n",
    "\n",
    "output_dir = '../data/landing/bus_hourly/bus_hourly.csv'\n",
    "\n",
    "urlretrieve(url, output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(output_dir).to_parquet('../data/landing/bus_hourly/bus_hourly.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing bus_hourly parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3199937"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_hourly = spark.read.parquet('../data/landing/bus_hourly/bus_hourly.parquet')\n",
    "\n",
    "\n",
    "bus_hourly.limit(20)\n",
    "\n",
    "bus_hourly.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotel Properties Citywide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/landing/hotels/hotels.csv',\n",
       " <http.client.HTTPMessage at 0x7fa0350df940>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if not os.path.exists(output_relative_dir + 'hotels'):\n",
    "    os.makedirs(output_relative_dir + 'hotels')\n",
    "\n",
    "url = 'https://data.cityofnewyork.us/api/views/tjus-cn27/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "output_dir = '../data/landing/hotels/hotels.csv'\n",
    "\n",
    "urlretrieve(url, output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(output_dir).to_parquet('../data/landing/hotels/hotels.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing pluto parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5519"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels = spark.read.parquet('../data/landing/hotels/hotels.parquet')\n",
    "\n",
    "\n",
    "hotels.limit(5)\n",
    "\n",
    "hotels.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC Airbnb Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/landing/airbnb/airbnb.csv',\n",
       " <http.client.HTTPMessage at 0x7fa036f80310>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if not os.path.exists(output_relative_dir + 'airbnb'):\n",
    "    os.makedirs(output_relative_dir + 'airbnb')\n",
    "\n",
    "url = 'http://data.insideairbnb.com/united-states/ny/new-york-city/2022-12-04/visualisations/listings.csv'\n",
    "\n",
    "output_dir = '../data/landing/airbnb/airbnb.csv'\n",
    "\n",
    "urlretrieve(url, output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(output_dir, dtype={\"license\": str}).to_parquet('../data/landing/airbnb/airbnb.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('host_id', LongType(), True), StructField('host_name', StringType(), True), StructField('neighbourhood_group', StringType(), True), StructField('neighbourhood', StringType(), True), StructField('latitude', DoubleType(), True), StructField('longitude', DoubleType(), True), StructField('room_type', StringType(), True), StructField('price', LongType(), True), StructField('minimum_nights', LongType(), True), StructField('number_of_reviews', LongType(), True), StructField('last_review', StringType(), True), StructField('reviews_per_month', DoubleType(), True), StructField('calculated_host_listings_count', LongType(), True), StructField('availability_365', LongType(), True), StructField('number_of_reviews_ltm', LongType(), True), StructField('license', StringType(), True)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing airbnb parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41533"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb = spark.read.parquet('../data/landing/airbnb/airbnb.parquet')\n",
    "\n",
    "\n",
    "airbnb.limit(5)\n",
    "\n",
    "airbnb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# unzip zip\n",
    "with zipfile.ZipFile('../data/landing/census/census.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../data/landing/census/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv('../data/landing/census/census_block_loc.csv').to_parquet('../data/landing/census/census_block_loc.parquet')\n",
    "\n",
    "pd.read_csv('../data/landing/census/nyc_census_tracts.csv').to_parquet('../data/landing/census/census_tracts.parquet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('host_id', LongType(), True), StructField('host_name', StringType(), True), StructField('neighbourhood_group', StringType(), True), StructField('neighbourhood', StringType(), True), StructField('latitude', DoubleType(), True), StructField('longitude', DoubleType(), True), StructField('room_type', StringType(), True), StructField('price', LongType(), True), StructField('minimum_nights', LongType(), True), StructField('number_of_reviews', LongType(), True), StructField('last_review', StringType(), True), StructField('reviews_per_month', DoubleType(), True), StructField('calculated_host_listings_count', LongType(), True), StructField('availability_365', LongType(), True), StructField('number_of_reviews_ltm', LongType(), True), StructField('license', StringType(), True)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airbnb.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing census parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") # fix timestamps loaded by spark\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38396"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_block_loc = spark.read.parquet('../data/landing/census/census_block_loc.parquet')\n",
    "\n",
    "\n",
    "census_block_loc.limit(5)\n",
    "\n",
    "census_block_loc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41533"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "census_tracts = spark.read.parquet('../data/landing/census/census_tracts.parquet')\n",
    "\n",
    "\n",
    "census_tracts.limit(5)\n",
    "\n",
    "census_tracts.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
